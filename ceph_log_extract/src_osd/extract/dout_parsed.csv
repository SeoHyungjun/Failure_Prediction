tag,filename,func,line,message
dout,osd/ECBackend.cc,<<handle_recovery_push>>,291," __func__ "" Out of space (failsafe) processing push request."""
dout,osd/ECBackend.cc,<<handle_recovery_push>>,306," __func__ "": Adding oid """
dout,osd/ECBackend.cc,<<handle_recovery_push>>,341," __func__ "": Removing oid """
dout,osd/ECBackend.cc,<<continue_recovery_op>>,541," __func__ "": continuing "" op"
dout,osd/ECBackend.cc,<<continue_recovery_op>>,566," __func__ "": canceling recovery op for obj "" op.hoid"
dout,osd/ECBackend.cc,<<continue_recovery_op>>,583," __func__ "": IDLE return "" op"
dout,osd/ECBackend.cc,<<continue_recovery_op>>,609," __func__ "": before_progress="" op.recovery_progress"
dout,osd/ECBackend.cc,<<continue_recovery_op>>,639," __func__ "": READING return "" op"
dout,osd/ECBackend.cc,<<continue_recovery_op>>,650," __func__ "": on_peer_recover on "" *i"
dout,osd/ECBackend.cc,<<continue_recovery_op>>,663," __func__ "": WRITING return "" op"
dout,osd/ECBackend.cc,<<continue_recovery_op>>,668," __func__ "": WRITING continue "" op"
dout,osd/ECBackend.cc,<<run_recovery_op>>,692," __func__ "": starting "" *i"
dout,osd/ECBackend.cc,<<recover_object>>,737," ""checking "" *i"
dout,osd/ECBackend.cc,<<recover_object>>,743," __func__ "": built op "" h->ops.back()"
dout,osd/ECBackend.cc,<<_handle_message>>,756," __func__ "": "" *_op->get_req()"
dout,osd/ECBackend.cc,<<handle_sub_write>>,899," __func__ "": removing object "" *i"
dout,osd/ECBackend.cc,<<handle_sub_write>>,910," __func__ "" missing before "" ge...)->get_log().get_missing().get_items()"
dout,osd/ECBackend.cc,<<handle_sub_write>>,913," __func__ "" is_missing "" pmissing.is_missing(op.soid)"
dout,osd/ECBackend.cc,<<handle_sub_write>>,915," "" add_next_event entry "" e"
dout,osd/ECBackend.cc,<<handle_sub_write>>,917," "" entry is_delete "" e.is_delete()"
dout,osd/ECBackend.cc,<<handle_sub_write>>,944," __func__ "" missing after"" get_...)->get_log().get_missing().get_items()"
dout,osd/ECBackend.cc,<<handle_sub_read>>,972," __func__ "": No hinfo for "" i->first"
dout,osd/ECBackend.cc,<<handle_sub_read>>,981," __func__ "" case1: reading the complete chunk/shard."""
dout,osd/ECBackend.cc,<<handle_sub_read>>,989," __func__ "" case2: going to do fragmented read."""
dout,osd/ECBackend.cc,<<handle_sub_read>>,1008," __func__ "": Error "" r"
dout,osd/ECBackend.cc,<<handle_sub_read>>,1012," __func__ "" read request="" j->g...< "" r="" r "" len="" bl.length()"
dout,osd/ECBackend.cc,<<handle_sub_read>>,1028," __func__ "": Checking hash of "" i->first"
dout,osd/ECBackend.cc,<<handle_sub_read>>,1034," __func__ "": Bad hash for "" i->first "" digest 0x"""
dout,osd/ECBackend.cc,<<handle_sub_read>>,1052," __func__ "": fulfilling attr request on """
dout,osd/ECBackend.cc,<<handle_sub_write_reply>>,1095," __func__ "" Calling on_all_commit on "" i->second"
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1110," __func__ "": reply "" op"
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1114," __func__ "": dropped "" op"
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1124," __func__ "" to_read skipping"""
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1151," __func__ "" to_read skipping"""
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1164," __func__ "" shard="" from "" error="" i->second"
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1189," __func__ "" have shard="" j->first.shard"
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1194," __func__ "" minimum_to_decode failed"""
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1219," __func__ "": Not ignoring errors, use one shard err="" err"
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1225," __func__ "" Error(s) ignored for "" iter->first"
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1235," __func__ "" Complete: "" rop"
dout,osd/ECBackend.cc,<<handle_sub_read_reply>>,1239," __func__ "" readop not complete: "" rop"
dout,osd/ECBackend.cc,<<filter_read_op>>,1319," __func__ "": canceling "" req"
dout,osd/ECBackend.cc,<<on_change>>,1362, __func__
dout,osd/ECBackend.cc,<<on_change>>,1378," __func__ "": cancelling "" i->second"
dout,osd/ECBackend.cc,<<do_read_op>>,1649," __func__ "": starting read "" op"
dout,osd/ECBackend.cc,<<do_read_op>>,1713," __func__ "": started "" op"
dout,osd/ECBackend.cc,<<start_rmw>>,1793," __func__ "": "" *op"
dout,osd/ECBackend.cc,<<try_state_to_reads>>,1807," __func__ "": blocking "" *op"
dout,osd/ECBackend.cc,<<try_state_to_reads>>,1815," __func__ "": invalidating cache after this op"""
dout,osd/ECBackend.cc,<<try_state_to_reads>>,1857," __func__ "": "" *op"
dout,osd/ECBackend.cc,<<try_reads_to_commit>>,1884," __func__ "": starting commit on "" *op"
dout,osd/ECBackend.cc,<<try_reads_to_commit>>,1885," __func__ "": "" cache"
dout,osd/ECBackend.cc,<<try_reads_to_commit>>,1930," __func__ "": "" cache"
dout,osd/ECBackend.cc,<<try_reads_to_commit>>,1931," __func__ "": written: "" written"
dout,osd/ECBackend.cc,<<try_reads_to_commit>>,1932," __func__ "": op: "" *op"
dout,osd/ECBackend.cc,<<try_reads_to_commit>>,1948," __func__ "": written_set: "" written_set"
dout,osd/ECBackend.cc,<<try_reads_to_commit>>,1953," __func__ "": "" hpair"
dout,osd/ECBackend.cc,<<try_finish_rmw>>,2041," __func__ "": "" *op"
dout,osd/ECBackend.cc,<<try_finish_rmw>>,2042," __func__ "": "" cache"
dout,osd/ECBackend.cc,<<try_finish_rmw>>,2073," __func__ "": clearing pipeline_state """
dout,osd/ECBackend.cc,<<send_all_remaining_reads>>,2333," __func__ "" have/error shards="" already_read"
dout,osd/ECBackend.cc,<<be_deep_scrub>>,2398," __func__ "" "" poid "" pos "" pos"
dout,osd/ECBackend.cc,<<be_deep_scrub>>,2430," __func__ "" "" poid "" got """
dout,osd/ECBackend.cc,<<be_deep_scrub>>,2436," __func__ "" "" poid "" got """
dout,osd/ECBackend.cc,<<be_deep_scrub>>,2452," ""_scan_list "" poid "" could not retrieve hash info"""
dout,osd/ECBackend.cc,<<be_deep_scrub>>,2460," ""_scan_list "" poid "" got incorrect size on read 0x"""
dout,osd/ECBackend.cc,<<be_deep_scrub>>,2471," ""_scan_list "" poid "" got incorrect hash on read 0x"""
dout,osd/OSD.cc,<<agent_entry>>,484," __func__ "" start"""
dout,osd/OSD.cc,<<agent_entry>>,489," __func__ "" empty queue"""
dout,osd/OSD.cc,<<agent_entry>>,495, __func__
dout,osd/OSD.cc,<<agent_entry>>,503," __func__ "" oids "" agent_oids"
dout,osd/OSD.cc,<<agent_entry>>,518," ""high_count "" flush_mode_high_count"
dout,osd/OSD.cc,<<agent_entry>>,523," __func__ "" "" pg->pg_id"
dout,osd/OSD.cc,<<agent_entry>>,537," __func__ "" finish"""
dout,osd/OSD.cc,<<promote_throttle_recalibrate>>,576," __func__ "" "" attempts "" attempts, promoted """
dout,osd/OSD.cc,<<promote_throttle_recalibrate>>,591," __func__ "" po "" po "" pb "" pb "" avg_size """
dout,osd/OSD.cc,<<promote_throttle_recalibrate>>,604," __func__ "" new_prob "" new_prob"
dout,osd/OSD.cc,<<promote_throttle_recalibrate>>,621," __func__ "" actual "" actual"
dout,osd/OSD.cc,<<check_full_status>>,695," __func__ "" cur ratio "" ratio"
dout,osd/OSD.cc,<<check_full_status>>,706," __func__ "" "" get_full_state_name(cur_state)"
dout,osd/OSD.cc,<<requeue_pg_temp>>,959," __func__ "" "" old_wanted "" + "" old_pending "" -> """
dout,osd/OSD.cc,<<send_pg_temp>>,978," ""send_pg_temp "" pg_temp_wanted"
dout,osd/OSD.cc,<<send_pg_created>>,998, __func__
dout,osd/OSD.cc,<<note_peer_epoch>>,1022," ""note_peer_epoch osd."" peer "" has "" e"
dout,osd/OSD.cc,<<note_peer_epoch>>,1025," ""note_peer_epoch osd."" peer "" has "" p->second "" >= "" e"
dout,osd/OSD.cc,<<note_peer_epoch>>,1029," ""note_peer_epoch osd."" peer "" now has "" e"
dout,osd/OSD.cc,<<forget_peer_epoch>>,1041," ""forget_peer_epoch osd."" peer "" as_of "" as_of"
dout,osd/OSD.cc,<<forget_peer_epoch>>,1045," ""forget_peer_epoch osd."" peer "" as_of "" as_of"
dout,osd/OSD.cc,<<should_share_map>>,1055," ""should_share_map """
dout,osd/OSD.cc,<<should_share_map>>,1063," ""client session last_sent_epoch: """
dout,osd/OSD.cc,<<should_share_map>>,1082," name "" "" con->get_peer_addr()"
dout,osd/OSD.cc,<<share_map>>,1099," ""share_map """
dout,osd/OSD.cc,<<share_map>>,1113," name "" has old map "" epoch"
dout,osd/OSD.cc,<<share_map>>,1124," name "" "" con->get_peer_addr()"
dout,osd/OSD.cc,<<share_map_peer>>,1145," ""share_map_peer "" con "" already has epoch "" pe"
dout,osd/OSD.cc,<<share_map_peer>>,1147," ""share_map_peer "" con "" don't know epoch, doing nothing"""
dout,osd/OSD.cc,<<can_inc_scrubs_pending>>,1160," __func__ "" "" scrubs_pending "" -> "" (scrubs_pending+1)"
dout,osd/OSD.cc,<<can_inc_scrubs_pending>>,1165," __func__ "" "" scrubs_pending "" + "" scrubs_active"
dout,osd/OSD.cc,<<inc_scrubs_pending>>,1178," ""inc_scrubs_pending "" scrubs_pending "" -> "" (scrubs_pending+1)"
dout,osd/OSD.cc,<<inc_scrubs_pending>>,1183," ""inc_scrubs_pending "" scrubs_pend...>= max "" cct->_conf->osd_max_scrubs"
dout,osd/OSD.cc,<<dec_scrubs_pending>>,1193," ""dec_scrubs_pending "" scrubs_pending "" -> "" (scrubs_pending-1)"
dout,osd/OSD.cc,<<inc_scrubs_active>>,1206," ""inc_scrubs_active "" (scrubs_active-1) "" -> "" scrubs_active"
dout,osd/OSD.cc,<<inc_scrubs_active>>,1211," ""inc_scrubs_active "" (scrubs_active-1) "" -> "" scrubs_active"
dout,osd/OSD.cc,<<dec_scrubs_active>>,1221," ""dec_scrubs_active "" scrubs_active "" -> "" (scrubs_active-1)"
dout,osd/OSD.cc,<<prepare_to_stop>>,1266," __func__ "" telling mon we are shutting down"""
dout,osd/OSD.cc,<<prepare_to_stop>>,1281," __func__ "" starting shutdown"""
dout,osd/OSD.cc,<<got_stop_ack>>,1290," __func__ "" starting shutdown"""
dout,osd/OSD.cc,<<got_stop_ack>>,1294," __func__ "" ignoring msg"""
dout,osd/OSD.cc,<<send_incremental_map>>,1334," ""send_incremental_map "" since "" -> "" to"
dout,osd/OSD.cc,<<send_incremental_map>>,1352," "" "" (to - since) "" > max "" cct->_conf->osd_map_share_max_epochs"
dout,osd/OSD.cc,<<_add_map_bl>>,1405," ""add_map_bl "" e "" "" bl.length() "" bytes"""
dout,osd/OSD.cc,<<_add_map_inc_bl>>,1416," ""add_map_inc_bl "" e "" "" bl.length() "" bytes"""
dout,osd/OSD.cc,<<get_deleted_pool_pg_num>>,1432," __func__ "" "" pool "" loading"""
dout,osd/OSD.cc,<<get_deleted_pool_pg_num>>,1441," __func__ "" "" pool "" got "" pi.get_pg_num()"
dout,osd/OSD.cc,<<try_get_map>>,1469," ""get_map "" epoch "" -cached"""
dout,osd/OSD.cc,<<try_get_map>>,1479," ""get_map "" epoch "" - miss, below lower bound"""
dout,osd/OSD.cc,<<try_get_map>>,1487," ""get_map "" epoch "" - loading and decoding "" map"
dout,osd/OSD.cc,<<try_get_map>>,1496," ""get_map "" epoch "" - return initial "" map"
dout,osd/OSD.cc,<<handle_misdirected_op>>,1553," __func__ "": "" *pg "" no longer have map for """
dout,osd/OSD.cc,<<handle_misdirected_op>>,1563," __func__ "": "" *pg "" primary changed since """
dout,osd/OSD.cc,<<handle_misdirected_op>>,1569," *pg "" misdirected op in "" m->get_map_epoch()"
dout,osd/OSD.cc,<<queue_for_snap_trim>>,1605," ""queueing "" *pg "" for snaptrim"""
dout,osd/OSD.cc,<<queue_for_pg_delete>>,1636," __func__ "" on "" pgid "" e "" e"
dout,osd/OSD.cc,<<mkfs>>,1731," "" have superblock"""
dout,osd/OSD.cc,<<asok_command>>,2242," ""triggering manual compaction"""
dout,osd/OSD.cc,<<asok_command>>,2247," ""finished manual compaction in """
dout,osd/OSD.cc,<<enable_disable_fuse>>,2322," __func__ "" disabling"""
dout,osd/OSD.cc,<<enable_disable_fuse>>,2336," __func__ "" enabling"""
dout,osd/OSD.cc,<<init>>,2403," ""init "" dev_path"
dout,osd/OSD.cc,<<init>>,2406," ""journal "" journal_path"
dout,osd/OSD.cc,<<init>>,2417," ""journal looks like "" (journal_is_rotational ? ""hdd"" : ""ssd"")"
dout,osd/OSD.cc,<<init>>,2422," ""boot"""
dout,osd/OSD.cc,<<init>>,2458," ""configured osd_max_object_name[space]_len looks ok"""
dout,osd/OSD.cc,<<init>>,2532," ""Upgrading superblock adding: "" diff"
dout,osd/OSD.cc,<<init>>,2542," ""init creating/touching snapmapper object"""
dout,osd/OSD.cc,<<init>>,2556," ""warning: got an error loading one or more classes: "" cpp_strerror(r)"
dout,osd/OSD.cc,<<init>>,2579," ""superblock: I am osd."" superblock.whoami"
dout,osd/OSD.cc,<<init>>,2580," ""using "" op_queue "" op queue with priority op cut off at "" "
dout,osd/OSD.cc,<<init>>,2670," ""ensuring pgs have consumed prior maps"""
dout,osd/OSD.cc,<<init>>,2673," ""done with init, starting boot process"""
dout,osd/OSD.cc,<<create_logger>>,2937," ""create_logger"""
dout,osd/OSD.cc,<<create_recoverystate_perf>>,3215," ""create_recoverystate_perf"""
dout,osd/OSD.cc,<<shutdown>>,3354," ""op sharded tp stopped"""
dout,osd/OSD.cc,<<shutdown>>,3358," ""command tp stopped"""
dout,osd/OSD.cc,<<shutdown>>,3360," ""stopping agent"""
dout,osd/OSD.cc,<<shutdown>>,3375," ""noting clean unmount in epoch "" osdmap->get_epoch()"
dout,osd/OSD.cc,<<shutdown>>,3403," "" kicking pg "" pg"
dout,osd/OSD.cc,<<shutdown>>,3426," ""syncing store"""
dout,osd/OSD.cc,<<shutdown>>,3430," ""flushing journal"""
dout,osd/OSD.cc,<<shutdown>>,3437," ""Store synced"""
dout,osd/OSD.cc,<<mon_cmd_maybe_osd_create>>,3466," __func__ "" cmd: "" cmd"
dout,osd/OSD.cc,<<update_crush_location>>,3503," __func__ "" osd_crush_update_on_start = false"""
dout,osd/OSD.cc,<<update_crush_location>>,3524," __func__ "" crush location is "" loc"
dout,osd/OSD.cc,<<update_crush_device_class>>,3544," __func__ "" osd_class_update_on_start = false"""
dout,osd/OSD.cc,<<update_crush_device_class>>,3555," __func__ "" no device class stored locally"""
dout,osd/OSD.cc,<<write_superblock>>,3575," ""write_superblock "" superblock"
dout,osd/OSD.cc,<<read_superblock>>,3596," ""read_superblock "" superblock"
dout,osd/OSD.cc,<<clear_temp_objects>>,3603, __func__
dout,osd/OSD.cc,<<clear_temp_objects>>,3612," "" clearing temps in "" *p "" pgid "" pgid"
dout,osd/OSD.cc,<<clear_temp_objects>>,3643," "" removing "" *p "" object "" *q"
dout,osd/OSD.cc,<<_make_pg>>,3711," __func__ "" "" pgid"
dout,osd/OSD.cc,<<register_pg>>,3792," __func__ "" "" pgid "" "" pg"
dout,osd/OSD.cc,<<unregister_pg>>,3804," __func__ "" "" pg->pg_id "" "" pg"
dout,osd/OSD.cc,<<unregister_pg>>,3807," __func__ "" "" pg->pg_id "" not found"""
dout,osd/OSD.cc,<<load_pgs>>,3845," ""load_pgs"""
dout,osd/OSD.cc,<<load_pgs>>,3860," ""load_pgs "" *it "" clearing temp"""
dout,osd/OSD.cc,<<load_pgs>>,3866," ""load_pgs ignoring unrecognized "" *it"
dout,osd/OSD.cc,<<load_pgs>>,3870," ""pgid "" pgid "" coll "" coll_t(pgid)"
dout,osd/OSD.cc,<<load_pgs>>,3915," ""load_pgs "" *it "" deleting dne"""
dout,osd/OSD.cc,<<load_pgs>>,3934," __func__ "" loaded "" *pg"
dout,osd/OSD.cc,<<load_pgs>>,3940," __func__ "" opened "" num "" pgs"""
dout,osd/OSD.cc,<<handle_pg_create_info>>,3950," __func__ "" hit max pg, dropping"""
dout,osd/OSD.cc,<<handle_pg_create_info>>,4002," __func__ "" new pg "" *pg"
dout,osd/OSD.cc,<<maybe_wait_for_max_pg>>,4025," __func__ "" withhold creation of pg "" pgid"
dout,osd/OSD.cc,<<resume_creating_pg>>,4058," __func__ "" pending_creates_from_mon """
dout,osd/OSD.cc,<<resume_creating_pg>>,4070," __func__ "" pg "" pg->first"
dout,osd/OSD.cc,<<resume_creating_pg>>,4085," __func__ "": resolicit pg creates from mon since """
dout,osd/OSD.cc,<<resume_creating_pg>>,4094," __func__ "": resolicit osdmap from mon since """
dout,osd/OSD.cc,<<resume_creating_pg>>,4102," __func__ "": re-subscribe osdmap(onetime) since """
dout,osd/OSD.cc,<<build_initial_pg_history>>,4122," __func__ "" "" pgid "" created "" created"
dout,osd/OSD.cc,<<build_initial_pg_history>>,4190," __func__ "" "" debug.str()"
dout,osd/OSD.cc,<<build_initial_pg_history>>,4191," __func__ "" "" *h "" "" *pi"
dout,osd/OSD.cc,<<_add_heartbeat_peer>>,4312," ""_add_heartbeat_peer: new peer osd."" p"
dout,osd/OSD.cc,<<_add_heartbeat_peer>>,4318," ""_add_heartbeat_peer: new peer osd."" p"
dout,osd/OSD.cc,<<_remove_heartbeat_peer>>,4333," "" removing heartbeat peer osd."" n"
dout,osd/OSD.cc,<<need_heartbeat_peer_update>>,4348," ""need_heartbeat_peer_update"""
dout,osd/OSD.cc,<<maybe_update_heartbeat_peers>>,4364," ""maybe_update_heartbeat_peers forcing update after "" dur "" seconds"""
dout,osd/OSD.cc,<<maybe_update_heartbeat_peers>>,4378," ""maybe_update_heartbeat_peers updating"""
dout,osd/OSD.cc,<<maybe_update_heartbeat_peers>>,4404," "" adding neighbor peer osd."" *p"
dout,osd/OSD.cc,<<maybe_update_heartbeat_peers>>,4429," "" adding random peer osd."" n"
dout,osd/OSD.cc,<<maybe_update_heartbeat_peers>>,4447," ""maybe_update_heartbeat_peers "" h....size() "" peers, extras "" extras"
dout,osd/OSD.cc,<<reset_heartbeat_peers>>,4453," ""reset_heartbeat_peers"""
dout,osd/OSD.cc,<<handle_osd_ping>>,4469," ""handle_osd_ping from "" m->get_source_inst()"
dout,osd/OSD.cc,<<handle_osd_ping>>,4502," ""Dropping heartbeat from "" from"
dout,osd/OSD.cc,<<handle_osd_ping>>,4512," ""Dropping heartbeat from "" from"
dout,osd/OSD.cc,<<handle_osd_ping>>,4520," ""internal heartbeat not healthy, dropping ping request"""
dout,osd/OSD.cc,<<handle_osd_ping>>,4556," ""handle_osd_ping got reply from osd."" from"
dout,osd/OSD.cc,<<handle_osd_ping>>,4567," ""handle_osd_ping got reply from osd."" from"
dout,osd/OSD.cc,<<handle_osd_ping>>,4582," ""handle_osd_ping canceling queued """
dout,osd/OSD.cc,<<handle_osd_ping>>,4589," ""handle_osd_ping canceling in-flight """
dout,osd/OSD.cc,<<handle_osd_ping>>,4612," ""handle_osd_ping "" m->get_source_inst()"
dout,osd/OSD.cc,<<heartbeat_entry>>,4633," ""heartbeat_entry sleeping for "" wait"
dout,osd/OSD.cc,<<heartbeat_entry>>,4637," ""heartbeat_entry woke up"""
dout,osd/OSD.cc,<<heartbeat_check>>,4654," ""heartbeat_check we haven't sent ping to osd."" p->first"
dout,osd/OSD.cc,<<heartbeat_check>>,4659," ""heartbeat_check osd."" p->first"
dout,osd/OSD.cc,<<heartbeat>>,4687," ""heartbeat"""
dout,osd/OSD.cc,<<heartbeat>>,4702," ""heartbeat: daily_loadavg "" daily_loadavg"
dout,osd/OSD.cc,<<heartbeat>>,4705," ""heartbeat checking stats"""
dout,osd/OSD.cc,<<heartbeat>>,4715," ""heartbeat: "" service.get_osd_stat()"
dout,osd/OSD.cc,<<heartbeat>>,4727," ""heartbeat sending ping to osd."" peer"
dout,osd/OSD.cc,<<heartbeat>>,4743," ""heartbeat lonely?"""
dout,osd/OSD.cc,<<heartbeat>>,4747," ""i have no heartbeat peers; checking mon for new map"""
dout,osd/OSD.cc,<<heartbeat>>,4752," ""heartbeat done"""
dout,osd/OSD.cc,<<heartbeat_reset>>,4769," ""heartbeat_reset failed hb con "" con "" for osd."" p->second.peer"
dout,osd/OSD.cc,<<heartbeat_reset>>,4788," ""heartbeat_reset failed hb con "" con "" for osd."" p->second.peer"
dout,osd/OSD.cc,<<heartbeat_reset>>,4793," ""heartbeat_reset closing (old) failed hb con "" con"
dout,osd/OSD.cc,<<tick>>,4808," ""tick"""
dout,osd/OSD.cc,<<tick_without_osd_lock>>,4826," ""tick_without_osd_lock"""
dout,osd/OSD.cc,<<tick_without_osd_lock>>,4861," __func__ "" max_waiting_epoch "" max_waiting_epoch"
dout,osd/OSD.cc,<<ms_handle_connect>>,5120," __func__ "" con "" con"
dout,osd/OSD.cc,<<ms_handle_connect>>,5125," __func__ "" on mon"""
dout,osd/OSD.cc,<<ms_handle_fast_connect>>,5168," "" new session (outgoing) "" s "" con="" s->con"
dout,osd/OSD.cc,<<ms_handle_fast_accept>>,5187," ""new session (incoming)"" s "" con="" con"
dout,osd/OSD.cc,<<ms_handle_reset>>,5200," ""ms_handle_reset con "" con "" session "" session"
dout,osd/OSD.cc,<<ms_handle_refused>>,5219," ""ms_handle_refused con "" con "" session "" session"
dout,osd/OSD.cc,<<start_boot>>,5258," ""not healthy; waiting to boot"""
dout,osd/OSD.cc,<<start_boot>>,5265, __func__
dout,osd/OSD.cc,<<start_boot>>,5267," ""start_boot - have maps "" superblock.oldest_map"
dout,osd/OSD.cc,<<_preboot>>,5284," __func__ "" _preboot mon has osdmaps """
dout,osd/OSD.cc,<<send_full_update>>,5338," __func__ "" want state "" s"
dout,osd/OSD.cc,<<start_waiting_for_healthy>>,5344," ""start_waiting_for_healthy"""
dout,osd/OSD.cc,<<_is_healthy>>,5355," ""is_healthy false -- internal heartbeat failed"""
dout,osd/OSD.cc,<<_is_healthy>>,5372," ""is_healthy false -- only "" up ""/"" num "" up peers (less than """
dout,osd/OSD.cc,<<_send_boot>>,5383," ""_send_boot"""
dout,osd/OSD.cc,<<_send_boot>>,5391," "" assuming cluster_addr ip matches client_addr"""
dout,osd/OSD.cc,<<_send_boot>>,5407," "" assuming hb_back_addr ip matches cluster_addr"""
dout,osd/OSD.cc,<<_send_boot>>,5423," "" assuming hb_front_addr ip matches client_addr"""
dout,osd/OSD.cc,<<_send_boot>>,5435," "" client_addr "" client_messenger->get_myaddr()"
dout,osd/OSD.cc,<<queue_want_up_thru>>,5509," ""queue_want_up_thru now "" want "" (was "" up_thru_wanted "")"""
dout,osd/OSD.cc,<<queue_want_up_thru>>,5515," ""queue_want_up_thru want "" want "" <= queued "" up_thru_wanted"
dout,osd/OSD.cc,<<send_alive>>,5528," ""send_alive up_thru currently "" up_thru "" want "" up_thru_wanted"
dout,osd/OSD.cc,<<send_alive>>,5530," ""send_alive want "" up_thru_wanted"
dout,osd/OSD.cc,<<request_full_map>>,5537," __func__ "" "" first "".."" last"
dout,osd/OSD.cc,<<got_full_map>>,5566," __func__ "" "" e "", nothing requested"""
dout,osd/OSD.cc,<<got_full_map>>,5570," __func__ "" "" e "", requested "" requested_full_first"
dout,osd/OSD.cc,<<got_full_map>>,5576," __func__ "" "" e "", requested "" requested_full_first"
dout,osd/OSD.cc,<<got_full_map>>,5584," __func__ "" "" e "", requested "" requested_full_first"
dout,osd/OSD.cc,<<requeue_failures>>,5600," __func__ "" "" old_queue "" + "" old_pending "" -> """
dout,osd/OSD.cc,<<send_beacon>>,5637," __func__ "" sending"""
dout,osd/OSD.cc,<<send_beacon>>,5647," __func__ "" not sending"""
dout,osd/OSD.cc,<<probe_smart>>,6266," ""probe_smart_device failed for /dev/"" dev"
dout,osd/OSD.cc,<<probe_smart_device>>,6307," ""smartctl output is: "" *result"
dout,osd/OSD.cc,<<heartbeat_dispatch>>,6320," ""heartbeat_dispatch "" m"
dout,osd/OSD.cc,<<heartbeat_dispatch>>,6324," ""ping from "" m->get_source_inst()"
dout,osd/OSD.cc,<<heartbeat_dispatch>>,6333," ""dropping unexpected message "" *m "" from "" m->get_source_inst()"
dout,osd/OSD.cc,<<ms_dispatch>>,6342," ""OSD::ms_dispatch: "" *m"
dout,osd/OSD.cc,<<ms_fast_dispatch>>,6444," ""ping from "" m->get_source()"
dout,osd/OSD.cc,<<ms_get_authorizer>>,6549," ""OSD::ms_get_authorizer type="" ceph_entity_type_name(dest_type)"
dout,osd/OSD.cc,<<ms_get_authorizer>>,6552," __func__ "" bailing, we are shutting down"""
dout,osd/OSD.cc,<<ms_verify_authorizer>>,6592," ""No AuthAuthorizeHandler found for protocol "" protocol"
dout,osd/OSD.cc,<<ms_verify_authorizer>>,6609," __func__ "" no rotating_keys (yet), denied"""
dout,osd/OSD.cc,<<ms_verify_authorizer>>,6619," "" new session "" s "" con="" s... "" addr="" s->con->get_peer_addr()"
dout,osd/OSD.cc,<<ms_verify_authorizer>>,6639," "" session "" s "" "" s->entity...aps "" s->caps "" '"" str ""'"""
dout,osd/OSD.cc,<<ms_verify_authorizer>>,6641," "" session "" s "" "" s->entity... failed to parse caps '"" str ""'"""
dout,osd/OSD.cc,<<ms_verify_authorizer>>,6642," ""parser returned "" ss.str()"
dout,osd/OSD.cc,<<do_waiters>>,6656," ""do_waiters -- start"""
dout,osd/OSD.cc,<<do_waiters>>,6662," ""do_waiters -- finish"""
dout,osd/OSD.cc,<<_dispatch>>,6678," ""_dispatch "" m "" "" *m"
dout,osd/OSD.cc,<<_dispatch>>,6706," ""no OSDMap, not booted"""
dout,osd/OSD.cc,<<handle_scrub>>,6722," ""handle_scrub "" *m"
dout,osd/OSD.cc,<<handle_scrub>>,6728," ""handle_scrub fsid "" m->fsid "" != "" monc->get_fsid()"
dout,osd/OSD.cc,<<handle_fast_scrub>>,6764," __func__ "" "" *m"
dout,osd/OSD.cc,<<handle_fast_scrub>>,6770," __func__ "" fsid "" m->fsid "" != "" monc->get_fsid()"
dout,osd/OSD.cc,<<scrub_random_backoff>>,6792," ""scrub_random_backoff lost coin flip, randomly backing off"""
dout,osd/OSD.cc,<<scrub_time_permit>>,6853," __func__ "" should run between week day "" cct->_conf->osd_scrub_begin_week_day"
dout,osd/OSD.cc,<<scrub_time_permit>>,6870," __func__ "" should run between "" cct->_conf->osd_scrub_begin_hour"
dout,osd/OSD.cc,<<scrub_time_permit>>,6874," __func__ "" should run between "" cct->_conf->osd_scrub_begin_hour"
dout,osd/OSD.cc,<<scrub_load_below_threshold>>,6885," __func__ "" couldn't read loadavgs\n"""
dout,osd/OSD.cc,<<scrub_load_below_threshold>>,6893," __func__ "" loadavg per cpu "" loadavg_per_cpu"
dout,osd/OSD.cc,<<scrub_load_below_threshold>>,6901," __func__ "" loadavg "" loadavgs[0]"
dout,osd/OSD.cc,<<scrub_load_below_threshold>>,6908," __func__ "" loadavg "" loadavgs[0]"
dout,osd/OSD.cc,<<sched_scrub>>,6923," __func__ "" not scheduling scrubs due to active recovery"""
dout,osd/OSD.cc,<<sched_scrub>>,6931," ""sched_scrub load_is_low="" (int )load_is_low"
dout,osd/OSD.cc,<<sched_scrub>>,6936," ""sched_scrub examine "" scrub.pgid "" at "" scrub.sched_time"
dout,osd/OSD.cc,<<sched_scrub>>,6940," ""sched_scrub "" scrub.pgid "" scheduled at "" scrub.sched_time"
dout,osd/OSD.cc,<<sched_scrub>>,6946," __func__ "" not scheduling scrub for "" scrub.pgid "" due to """
dout,osd/OSD.cc,<<sched_scrub>>,6954," ""sched_scrub scrubbing "" scrub.pgid "" at "" scrub.sched_time"
dout,osd/OSD.cc,<<sched_scrub>>,6965," ""sched_scrub done"""
dout,osd/OSD.cc,<<trim_maps>>,7129," "" removing old osdmap epoch "" e"
dout,osd/OSD.cc,<<handle_osd_map>>,7172," ""handle_osd_map fsid "" m->fsid "" != """
dout,osd/OSD.cc,<<handle_osd_map>>,7178," ""ignoring osdmap until we have initialized"""
dout,osd/OSD.cc,<<handle_osd_map>>,7187," ""got osd map from Session "" session"
dout,osd/OSD.cc,<<handle_osd_map>>,7202," ""handle_osd_map epochs ["" first "","" last ""], i have """
dout,osd/OSD.cc,<<handle_osd_map>>,7219," "" no new maps here, dropping"""
dout,osd/OSD.cc,<<handle_osd_map>>,7227," ""handle_osd_map message skips epochs """
dout,osd/OSD.cc,<<handle_osd_map>>,7260," __func__ "" waiting for pgs to consume "" need"
dout,osd/OSD.cc,<<handle_osd_map>>,7285," ""handle_osd_map got full map for epoch "" e"
dout,osd/OSD.cc,<<handle_osd_map>>,7301," ""handle_osd_map got inc map for epoch "" e"
dout,osd/OSD.cc,<<handle_osd_map>>,7337," ""got incremental "" e"
dout,osd/OSD.cc,<<handle_osd_map>>,7341," ""my encoded map was:\n"";"
dout,osd/OSD.cc,<<handle_osd_map>>,7365," __func__ "" still missing full maps "" requested_full_first"
dout,osd/OSD.cc,<<handle_osd_map>>,7396," __func__ "" recording final pg_pool_t for pool """
dout,osd/OSD.cc,<<_committed_osd_maps>>,7429," __func__ "" "" first "".."" last"
dout,osd/OSD.cc,<<_committed_osd_maps>>,7431," __func__ "" bailing, we are shutting down"""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7436," __func__ "" bailing, we are shutting down"""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7447," "" advance to epoch "" cur"
dout,osd/OSD.cc,<<_committed_osd_maps>>,7481," __func__ "" NOUP flag changed in "" newmap->get_epoch()"
dout,osd/OSD.cc,<<_committed_osd_maps>>,7502," ""up_epoch is "" up_epoch"
dout,osd/OSD.cc,<<_committed_osd_maps>>,7505," ""boot_epoch is "" boot_epoch"
dout,osd/OSD.cc,<<_committed_osd_maps>>,7519," ""state: booting -> active"""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7532," ""map says i do not exist. shutting down."""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7598," __func__ "" marked down """
dout,osd/OSD.cc,<<_committed_osd_maps>>,7625," __func__ "" marked down:"""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7633," __func__ "" marked down:"""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7641," __func__ "" marked down:"""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7664," "" not yet active; waiting for peering work to drain"""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7678," ""handle_osd_ping canceling in-flight failure report for osd."""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7685," __func__ "" shutdown OSD via async signal"""
dout,osd/OSD.cc,<<_committed_osd_maps>>,7689," "" msg say newest map is "" m->newest_map"
dout,osd/OSD.cc,<<check_osdmap_features>>,7720," ""crush map has features "" features"
dout,osd/OSD.cc,<<check_osdmap_features>>,7731," ""crush map has features "" features"
dout,osd/OSD.cc,<<check_osdmap_features>>,7744," ""crush map has features "" features"
dout,osd/OSD.cc,<<check_osdmap_features>>,7751," __func__ "" enabling on-disk ERASURE CODES compat feature"""
dout,osd/OSD.cc,<<advance_pg>>,7812," __func__ "" missing map "" next_epoch"
dout,osd/OSD.cc,<<consume_map>>,7852," ""consume_map version "" osdmap->get_epoch()"
dout,osd/OSD.cc,<<activate_map>>,7943," ""activate_map version "" osdmap->get_epoch()"
dout,osd/OSD.cc,<<activate_map>>,7946," "" osdmap flagged full, doing onetime osdmap subscribe"""
dout,osd/OSD.cc,<<activate_map>>,7953," ""pausing recovery (NORECOVER flag set)"""
dout,osd/OSD.cc,<<activate_map>>,7958," ""unpausing recovery (NORECOVER flag unset)"""
dout,osd/OSD.cc,<<require_mon_peer>>,7972," ""require_mon_peer received from non-mon """
dout,osd/OSD.cc,<<require_mon_or_mgr_peer>>,7984," ""require_mon_or_mgr_peer received from non-mon, non-mgr """
dout,osd/OSD.cc,<<require_osd_peer>>,7995," ""require_osd_peer received from non-osd """
dout,osd/OSD.cc,<<require_self_aliveness>>,8007," ""from pre-up epoch "" epoch "" < "" up_epoch"
dout,osd/OSD.cc,<<require_self_aliveness>>,8012," ""still in boot state, dropping message "" *m"
dout,osd/OSD.cc,<<require_same_peer_instance>>,8026," ""from dead osd."" from "", marking down, """
dout,osd/OSD.cc,<<require_same_or_newer_map>>,8057," ""require_same_or_newer_map "" epoch"
dout,osd/OSD.cc,<<require_same_or_newer_map>>,8064," ""waiting for newer map epoch "" epoch"
dout,osd/OSD.cc,<<handle_pg_create>>,8144," ""handle_pg_create "" *m"
dout,osd/OSD.cc,<<handle_pg_create>>,8166," ""ignoring pg on deleted pool "" on"
dout,osd/OSD.cc,<<handle_pg_create>>,8170," ""mkpg "" on "" e"" created ""@"" ci->second"
dout,osd/OSD.cc,<<handle_pg_create>>,8180," ""mkpg "" on "" not acting_primary ("" acting_primary"
dout,osd/OSD.cc,<<handle_pg_create>>,8197," __func__ "": got obsolete pg create on pgid """
dout,osd/OSD.cc,<<dispatch_context>>,8263," __func__ "" not up in osdmap"""
dout,osd/OSD.cc,<<dispatch_context>>,8265," __func__ "" not active"""
dout,osd/OSD.cc,<<handle_fast_pg_create>>,8393," __func__ "" "" *m "" from "" m->get_source()"
dout,osd/OSD.cc,<<handle_fast_pg_create>>,8402," __func__ "" "" pgid "" e"" created"
dout,osd/OSD.cc,<<handle_fast_pg_query>>,8443," __func__ "" "" *m "" from "" m->get_source()"
dout,osd/OSD.cc,<<handle_fast_pg_notify>>,8468," __func__ "" "" *m "" from "" m->get_source()"
dout,osd/OSD.cc,<<handle_fast_pg_info>>,8501," __func__ "" "" *m "" from "" m->get_source()"
dout,osd/OSD.cc,<<handle_fast_pg_remove>>,8524," __func__ "" "" *m "" from "" m->get_source()"
dout,osd/OSD.cc,<<handle_fast_force_recovery>>,8542," __func__ "" "" *m"
dout,osd/OSD.cc,<<handle_pg_query_nopg>>,8589," __func__ "" "" pgid"
dout,osd/OSD.cc,<<handle_pg_query_nopg>>,8609," "" pg "" pgid "" dne, and pg has changed in """
dout,osd/OSD.cc,<<handle_pg_query_nopg>>,8615," "" pg "" pgid "" dne"""
dout,osd/OSD.cc,<<_maybe_queue_recovery>>,8657," __func__ "" starting "" to_start"
dout,osd/OSD.cc,<<_recover_now>>,8670," __func__ "" defer until "" defer_recovery_until"
dout,osd/OSD.cc,<<_recover_now>>,8675," __func__ "" paused"""
dout,osd/OSD.cc,<<_recover_now>>,8681," __func__ "" active "" recovery_ops_active"
dout,osd/OSD.cc,<<do_recovery>>,8712," ""do_recovery wake up at """
dout,osd/OSD.cc,<<do_recovery>>,8729," ""Recovery event scheduled at """
dout,osd/OSD.cc,<<do_recovery>>,8745," ""do_recovery starting "" reserved_pushes "" "" *pg"
dout,osd/OSD.cc,<<do_recovery>>,8747," "" active was "" service.recovery_oids[pg->pg_id]"
dout,osd/OSD.cc,<<do_recovery>>,8751," ""do_recovery started "" started ""/"" reserved_pushes"
dout,osd/OSD.cc,<<start_recovery_op>>,8770," ""start_recovery_op "" *pg "" "" soid"
dout,osd/OSD.cc,<<start_recovery_op>>,8777," "" active was "" recovery_oids[pg->pg_id]"
dout,osd/OSD.cc,<<finish_recovery_op>>,8786," ""finish_recovery_op "" *pg "" "" soid"
dout,osd/OSD.cc,<<finish_recovery_op>>,8796," "" active oids was "" recovery_oids[pg->pg_id]"
dout,osd/OSD.cc,<<release_reserved_pushes>>,8812," __func__ ""("" pushes ""), recovery_ops_reserved """
dout,osd/OSD.cc,<<enqueue_op>>,8836," ""enqueue_op "" op "" prio "" op->get_req()->get_priority()"
dout,osd/OSD.cc,<<enqueue_peering_evt>>,8858," __func__ "" "" pgid "" "" evt->get_desc()"
dout,osd/OSD.cc,<<enqueue_peering_evt_front>>,8871," __func__ "" "" pgid "" "" evt->get_desc()"
dout,osd/OSD.cc,<<dequeue_op>>,8895," ""dequeue_op "" op "" prio "" op->get_req()->get_priority()"
dout,osd/OSD.cc,<<dequeue_op>>,8919," ""dequeue_op "" op "" finish"""
dout,osd/OSD.cc,<<get_latest_osdmap>>,9143," __func__ "" -- start"""
dout,osd/OSD.cc,<<get_latest_osdmap>>,9149," __func__ "" -- finish"""
dout,osd/OSD.cc,<<init_op_flags>>,9260," ""class "" cname "" method "" mname "" """
dout,osd/OSD.cc,<<_attach_pg>>,9350," pg->pg_id "" "" pg"
dout,osd/OSD.cc,<<_detach_pg>>,9362," slot->pg->pg_id "" "" slot->pg"
dout,osd/OSD.cc,<<update_pg_epoch>>,9378," ""min was "" pg_slots_by_epoch.begin()->epoch"
dout,osd/OSD.cc,<<update_pg_epoch>>,9381," slot->pg->pg_id "" "" slot->epoch "" -> "" e"
dout,osd/OSD.cc,<<update_pg_epoch>>,9384," ""min is now "" pg_slots_by_epoch.begin()->epoch"
dout,osd/OSD.cc,<<wait_min_pg_epoch>>,9407," need "" waiting on """
dout,osd/OSD.cc,<<consume_map>>,9437, new_osdmap->get_epoch()
dout,osd/OSD.cc,<<consume_map>>,9447," __func__ "" "" pgid"
dout,osd/OSD.cc,<<consume_map>>,9449," __func__ "" "" pgid"
dout,osd/OSD.cc,<<consume_map>>,9457," __func__ "" "" pgid"
dout,osd/OSD.cc,<<consume_map>>,9468," __func__ "" "" pgid "" maps to us, keeping"""
dout,osd/OSD.cc,<<consume_map>>,9476," __func__ "" "" pgid"
dout,osd/OSD.cc,<<consume_map>>,9491," __func__ "" "" pgid "" empty, pruning"""
dout,osd/OSD.cc,<<_wake_pg_slot>>,9509," __func__ "" "" pgid"
dout,osd/OSD.cc,<<register_and_wake_split_child>>,9617," pg->pg_id "" "" pg"
dout,osd/OSD.cc,<<unprime_split_children>>,9638," __func__ "" parent "" parent "" clearing "" i.first"
dout,osd/OSD.cc,<<_add_slot_waiter>>,9661," __func__ "" "" pgid"
dout,osd/OSD.cc,<<_add_slot_waiter>>,9667," __func__ "" "" pgid"
dout,osd/OSD.cc,<<_process>>,9688," __func__ "" empty q, waiting"""
dout,osd/OSD.cc,<<_process>>,9701," __func__ "" need return immediately"""
dout,osd/OSD.cc,<<_process>>,9718," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9725," __func__ "" "" slot->to_process.back()"
dout,osd/OSD.cc,<<_process>>,9746," __func__ "" slot "" token "" no longer there"""
dout,osd/OSD.cc,<<_process>>,9756," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9763," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9773," __func__ "" slot "" token "" no longer attached to """
dout,osd/OSD.cc,<<_process>>,9780," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9791," __func__ "" "" qi "" pg "" pg"
dout,osd/OSD.cc,<<_process>>,9800," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9804," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9817," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9821," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9836," __func__ "" ignored create on "" qi"
dout,osd/OSD.cc,<<_process>>,9839," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9843," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9849," __func__ "" "" token"
dout,osd/OSD.cc,<<_process>>,9854," __func__ "" "" token"
dout,osd/OSD.cc,<<_enqueue>>,9940," __func__ "" "" item"
dout,osd/OSD.cc,<<_enqueue_front>>,9971, __func__
dout,osd/OSD.cc,<<_enqueue_front>>,9975," __func__ "" "" item"
dout,osd/PG.cc,<<get_with_id>>,188," __func__ "": "" info.pgid "" got id "" id "" (new) ref=="" ref"
dout,osd/PG.cc,<<put_with_id>>,196," __func__ "": "" info.pgid "" put id "" id "" (current) ref=="" ref"
dout,osd/PG.cc,<<dump_live_ids>>,209," ""\t"" __func__ "": "" info.pgid "" live ids:"""
dout,osd/PG.cc,<<dump_live_ids>>,213," ""\t\tid: "" *i"
dout,osd/PG.cc,<<dump_live_ids>>,215," ""\t"" __func__ "": "" info.pgid "" live tags:"""
dout,osd/PG.cc,<<dump_live_ids>>,219," ""\t\tid: "" *i"
dout,osd/PG.cc,<<lock>>,383," ""lock"""
dout,osd/PG.cc,<<proc_master_log>>,407," ""proc_master_log for osd."" from "": """
dout,osd/PG.cc,<<proc_master_log>>,417," "" peer osd."" from "" now "" oinfo "" "" omissing"
dout,osd/PG.cc,<<proc_replica_log>>,442," ""proc_replica_log for osd."" from "": """
dout,osd/PG.cc,<<proc_replica_log>>,448," "" peer osd."" from "" now "" oinfo "" "" omissing"
dout,osd/PG.cc,<<proc_replica_log>>,455," "" after missing "" i->first "" need "" i->second.need"
dout,osd/PG.cc,<<proc_replica_info>>,466," "" got dup osd."" from "" info "" oinfo "", identical to ours"""
dout,osd/PG.cc,<<proc_replica_info>>,471," "" got info "" oinfo "" from down osd."" from"
dout,osd/PG.cc,<<proc_replica_info>>,476," "" got osd."" from "" "" oinfo"
dout,osd/PG.cc,<<proc_replica_info>>,485," "" osd."" from "" has stray content: "" oinfo"
dout,osd/PG.cc,<<needs_recovery>>,845," __func__ "" primary has "" missing.num_missing()"
dout,osd/PG.cc,<<needs_recovery>>,858," __func__ "" osd."" peer "" doesn't have missing set"""
dout,osd/PG.cc,<<needs_recovery>>,863," __func__ "" osd."" peer "" has """
dout,osd/PG.cc,<<needs_recovery>>,869," __func__ "" is recovered"""
dout,osd/PG.cc,<<needs_backfill>>,885," __func__ "" osd."" peer "" has...backfill "" pi->second.last_backfill"
dout,osd/PG.cc,<<needs_backfill>>,890," __func__ "" does not need backfill"""
dout,osd/PG.cc,<<adjust_need_up_thru>>,947," ""adjust_need_up_thru now "" up_thru "", need_up_thru now false"""
dout,osd/PG.cc,<<remove_down_peer_info>>,961," "" dropping down osd."" p->first "" info "" p->second"
dout,osd/PG.cc,<<all_unfound_are_queried_or_lost>>,1002," ""all_unfound_are_queried_or_lost all of might_have_unfound "" might_have_unfound"
dout,osd/PG.cc,<<build_prior>>,1050," ""up_thru "" get_osdmap()->get_up_thru(osd->whoami)"
dout,osd/PG.cc,<<build_prior>>,1055," ""up_thru "" get_osdmap()->get_up_thru(osd->whoami)"
dout,osd/PG.cc,<<clear_primary_state>>,1066," ""clear_primary_state"""
dout,osd/PG.cc,<<choose_acting>>,1617," __func__ "" all_info osd."" p->first "" "" p->second"
dout,osd/PG.cc,<<choose_acting>>,1626," __func__ "" no suitable info found (incomplete backfills?),"""
dout,osd/PG.cc,<<choose_acting>>,1632," __func__ "" failed"""
dout,osd/PG.cc,<<choose_acting>>,1669, ss.str()
dout,osd/PG.cc,<<choose_acting>>,1685," __func__ "" want "" want "" != acting "" acting"
dout,osd/PG.cc,<<choose_acting>>,1701," ""acting_recovery_backfill is "" acting_recovery_backfill"
dout,osd/PG.cc,<<choose_acting>>,1719," ""choose_acting want="" want "" backfill_targets="""
dout,osd/PG.cc,<<build_might_have_unfound>>,1738, __func__
dout,osd/PG.cc,<<build_might_have_unfound>>,1752," __func__ "": built "" might_have_unfound"
dout,osd/PG.cc,<<op_has_sufficient_caps>>,2096," ""op_has_sufficient_caps: no session for op "" *req"
dout,osd/PG.cc,<<op_has_sufficient_caps>>,2114," ""op_has_sufficient_caps """
dout,osd/PG.cc,<<_activate_committed>>,2132," ""_activate_committed "" epoch"
dout,osd/PG.cc,<<_activate_committed>>,2137," ""_activate_committed "" epoch"
dout,osd/PG.cc,<<_activate_committed>>,2146," ""_activate_committed "" epoch "" telling primary"""
dout,osd/PG.cc,<<_activate_committed>>,2169," __func__ "" flushes in progress, moving """
dout,osd/PG.cc,<<all_activated_and_committed>>,2189," ""all_activated_and_committed"""
dout,osd/PG.cc,<<requeue_scrub>>,2215," __func__ "": already queued"""
dout,osd/PG.cc,<<requeue_scrub>>,2218," __func__ "": queueing"""
dout,osd/PG.cc,<<queue_recovery>>,2228," ""queue_recovery -- not primary or not peered """
dout,osd/PG.cc,<<queue_recovery>>,2231," ""queue_recovery -- already queued"""
dout,osd/PG.cc,<<queue_recovery>>,2233," ""queue_recovery -- queuing"""
dout,osd/PG.cc,<<set_force_recovery>>,2292," __func__ "" set"""
dout,osd/PG.cc,<<set_force_recovery>>,2298," __func__ "" clear"""
dout,osd/PG.cc,<<set_force_backfill>>,2314," __func__ "" set"""
dout,osd/PG.cc,<<set_force_backfill>>,2320," __func__ "" clear"""
dout,osd/PG.cc,<<get_recovery_priority>>,2354," __func__ "" recovery priority for ... is "" ret "", state is "" state"
dout,osd/PG.cc,<<finish_recovery>>,2404," ""finish_recovery"""
dout,osd/PG.cc,<<_finish_recovery>>,2424," ""_finish_recovery"""
dout,osd/PG.cc,<<_finish_recovery>>,2431," ""_finish_recovery requeueing for scrub"""
dout,osd/PG.cc,<<_finish_recovery>>,2437," ""_finish_recovery -- stale"""
dout,osd/PG.cc,<<start_recovery_op>>,2444," ""start_recovery_op "" soid"
dout,osd/PG.cc,<<finish_recovery_op>>,2459," ""finish_recovery_op "" soid"
dout,osd/PG.cc,<<add_backoff>>,2598," __func__ "" session "" s "" added "" *b"
dout,osd/PG.cc,<<release_backoffs>>,2612," __func__ "" ["" begin "","" end "")"""
dout,osd/PG.cc,<<release_backoffs>>,2619," __func__ "" ? "" r "" "" p->first"
dout,osd/PG.cc,<<release_backoffs>>,2625," __func__ "" checking "" p->first"
dout,osd/PG.cc,<<release_backoffs>>,2629," __func__ "" checking "" *q"
dout,osd/PG.cc,<<release_backoffs>>,2647," __func__ "" "" *b"
dout,osd/PG.cc,<<clear_backoffs>>,2674," __func__ "" """
dout,osd/PG.cc,<<clear_backoffs>>,2683," __func__ "" "" *b"
dout,osd/PG.cc,<<rm_backoff>>,2701," __func__ "" "" *b"
dout,osd/PG.cc,<<clear_recovery_state>>,2720," ""clear_recovery_state"""
dout,osd/PG.cc,<<cancel_recovery>>,2743," ""cancel_recovery"""
dout,osd/PG.cc,<<purge_strays>>,2750," ""purge_strays "" stray_set"
dout,osd/PG.cc,<<purge_strays>>,2758," ""sending PGRemove to osd."" *p"
dout,osd/PG.cc,<<purge_strays>>,2766," ""not sending PGRemove to down osd."" *p"
dout,osd/PG.cc,<<update_heartbeat_peers>>,2827," ""update_heartbeat_peers "" heartbeat_peers "" unchanged"""
dout,osd/PG.cc,<<update_heartbeat_peers>>,2829," ""update_heartbeat_peers "" heartbeat_peers "" -> "" new_peers"
dout,osd/PG.cc,<<_update_calc_stats>>,2901," __func__ "" actingset "" actingset "" upset """
dout,osd/PG.cc,<<_update_calc_stats>>,2903," __func__ "" acting "" acting "" up "" up"
dout,osd/PG.cc,<<_update_calc_stats>>,2932," __func__ "" shard "" pg_whoami"
dout,osd/PG.cc,<<_update_calc_stats>>,2952," __func__ "" no peer_missing found for "" peer.first"
dout,osd/PG.cc,<<_update_calc_stats>>,2965," __func__ "" shard "" peer.first"
dout,osd/PG.cc,<<_update_calc_stats>>,2981," __func__ "" ml "" ml.second "" ... upset.size() "" up "" ml.first.up"
dout,osd/PG.cc,<<_update_calc_stats>>,2988," __func__ "" shard "" sml.first ...< "" missing shards "" missing_shards"
dout,osd/PG.cc,<<_update_calc_stats>>,3002," __func__ "" missing based degraded "" degraded"
dout,osd/PG.cc,<<_update_calc_stats>>,3003," __func__ "" missing based misplaced "" misplaced"
dout,osd/PG.cc,<<_update_calc_stats>>,3061," __func__ "" missing shard "" std...) "" missing= "" std::get<0>(item)"
dout,osd/PG.cc,<<_update_calc_stats>>,3063," __func__ "" acting shard "" std:...) "" missing= "" std::get<0>(item)"
dout,osd/PG.cc,<<_update_calc_stats>>,3105," __func__ "" extra acting misplaced "" extra_misplaced"
dout,osd/PG.cc,<<_update_calc_stats>>,3110," __func__ "" degraded "" degraded (estimate ? "" (est)"": """")"
dout,osd/PG.cc,<<_update_calc_stats>>,3111," __func__ "" misplaced "" misplaced (estimate ? "" (est)"": """")"
dout,osd/PG.cc,<<publish_stats_to_osd>>,3198," __func__ "" reporting purged_snaps """
dout,osd/PG.cc,<<publish_stats_to_osd>>,3204," ""publish_stats_to_osd "" pg_stats_publish.reported_epoch"
dout,osd/PG.cc,<<publish_stats_to_osd>>,3228," ""publish_stats_to_osd "" pg_stats_publish.reported_epoch"
dout,osd/PG.cc,<<clear_publish_stats>>,3236," ""clear_stats"""
dout,osd/PG.cc,<<upgrade>>,3316," __func__ "" "" info_struct_v "" -> "" latest_struct_v"
dout,osd/PG.cc,<<trim_log>>,3553," __func__ "" to "" pg_trim_to"
dout,osd/PG.cc,<<add_log_entry>>,3593," ""add_log_entry "" e"
dout,osd/PG.cc,<<get_corrupt_pg_log_name>>,3678," ""strftime failed"""
dout,osd/PG.cc,<<requeue_op>>,3907," __func__ "" "" op "" (waiting_for_map "" p->first "")"""
dout,osd/PG.cc,<<requeue_op>>,3911," __func__ "" "" op"
dout,osd/PG.cc,<<requeue_map_waiters>>,3939," __func__ "" "" p->first "" front op """
dout,osd/PG.cc,<<requeue_map_waiters>>,3944," __func__ "" "" p->first "" "" p->second"
dout,osd/PG.cc,<<sched_scrub>>,4006," __func__ "": time_for_deep="" ti...< "" deep_coin_flip="" deep_coin_flip"
dout,osd/PG.cc,<<sched_scrub>>,4041," __func__ "": auto repair with deep scrubbing"""
dout,osd/PG.cc,<<sched_scrub>>,4054," __func__ "": reserved locally, reserving replicas"""
dout,osd/PG.cc,<<sched_scrub>>,4059," __func__ "": failed to reserve locally"""
dout,osd/PG.cc,<<sched_scrub>>,4065," ""sched_scrub: failed, a peer declined"""
dout,osd/PG.cc,<<sched_scrub>>,4070," ""sched_scrub: success, reserved self and replicas"""
dout,osd/PG.cc,<<sched_scrub>>,4072," ""sched_scrub: scrub will be deep"""
dout,osd/PG.cc,<<sched_scrub>>,4096," ""sched_scrub: reserved "" scrubber...rved_peers "", waiting for replicas"""
dout,osd/PG.cc,<<do_replica_scrub_map>>,4141," __func__ "" "" *m"
dout,osd/PG.cc,<<do_replica_scrub_map>>,4143," __func__ "" discarding old from """
dout,osd/PG.cc,<<do_replica_scrub_map>>,4149," __func__ "" scrub isn't active"""
dout,osd/PG.cc,<<do_replica_scrub_map>>,4157," ""map version is """
dout,osd/PG.cc,<<do_replica_scrub_map>>,4161," __func__ "" waiting_on_whom was "" scrubber.waiting_on_whom"
dout,osd/PG.cc,<<do_replica_scrub_map>>,4166," __func__ "" replica was preempted, setting flag"""
dout,osd/PG.cc,<<_request_scrub_map>>,4182," ""scrub requesting scrubmap from osd."" replica"
dout,osd/PG.cc,<<handle_scrub_reserve_request>>,4200," __func__ "" "" *op->get_req()"
dout,osd/PG.cc,<<handle_scrub_reserve_request>>,4203," __func__ "" ignoring reserve request: Already reserved"""
dout,osd/PG.cc,<<handle_scrub_reserve_request>>,4211," __func__ "": failed to reserve remotely"""
dout,osd/PG.cc,<<handle_scrub_reserve_grant>>,4226," __func__ "" "" *op->get_req()"
dout,osd/PG.cc,<<handle_scrub_reserve_grant>>,4229," ""ignoring obsolete scrub reserve reply"""
dout,osd/PG.cc,<<handle_scrub_reserve_grant>>,4233," "" already had osd."" from "" reserved"""
dout,osd/PG.cc,<<handle_scrub_reserve_grant>>,4235," "" osd."" from "" scrub reserve = success"""
dout,osd/PG.cc,<<handle_scrub_reserve_reject>>,4243," __func__ "" "" *op->get_req()"
dout,osd/PG.cc,<<handle_scrub_reserve_reject>>,4246," ""ignoring obsolete scrub reserve reply"""
dout,osd/PG.cc,<<handle_scrub_reserve_reject>>,4250," "" already had osd."" from "" reserved"""
dout,osd/PG.cc,<<handle_scrub_reserve_reject>>,4253," "" osd."" from "" scrub reserve = fail"""
dout,osd/PG.cc,<<handle_scrub_reserve_release>>,4261," __func__ "" "" *op->get_req()"
dout,osd/PG.cc,<<scrub_reserve_replicas>>,4315," ""scrub requesting reserve from osd."" *i"
dout,osd/PG.cc,<<scrub_unreserve_replicas>>,4332," ""scrub requesting unreserve from osd."" *i"
dout,osd/PG.cc,<<_scan_snaps>>,4373," __func__ "" start"""
dout,osd/PG.cc,<<_scan_snaps>>,4381," __func__ "" "" hoid"
dout,osd/PG.cc,<<build_scrub_map_chunk>>,4527," __func__ "" ["" start "","" end "") """
dout,osd/PG.cc,<<build_scrub_map_chunk>>,4544," ""objects_list_range error: "" pos.ret"
dout,osd/PG.cc,<<build_scrub_map_chunk>>,4564," __func__ "" finishing"""
dout,osd/PG.cc,<<build_scrub_map_chunk>>,4574," __func__ "" done, got "" map.objects.size() "" items"""
dout,osd/PG.cc,<<replica_scrub>>,4652," ""replica_scrub"""
dout,osd/PG.cc,<<replica_scrub>>,4655," ""replica_scrub discarding old replica_scrub from """
dout,osd/PG.cc,<<replica_scrub>>,4663," ""waiting for active pushes to finish"""
dout,osd/PG.cc,<<scrub>>,4700," __func__ "" state is INACTIVE|NEW_CHUNK, sleeping"""
dout,osd/PG.cc,<<scrub>>,4749," ""scrub -- not primary or active or not clean"""
dout,osd/PG.cc,<<scrub>>,4762," ""starting a new chunky scrub"""
dout,osd/PG.cc,<<chunky_scrub>>,4851," ""scrub pg changed, aborting"""
dout,osd/PG.cc,<<chunky_scrub>>,4862," ""scrub state "" Scrubber::state_string(scrubber.state)"
dout,osd/PG.cc,<<chunky_scrub>>,4867," ""scrub start"""
dout,osd/PG.cc,<<chunky_scrub>>,4914," __func__ "" preempted, "" scrubber.preempt_left"
dout,osd/PG.cc,<<chunky_scrub>>,4975," __func__ "": scrub blocked somewhere in range """
dout,osd/PG.cc,<<chunky_scrub>>,5015," ""wait for pushes to apply"""
dout,osd/PG.cc,<<chunky_scrub>>,5023," ""wait for EC read/modify/writes to queue"""
dout,osd/PG.cc,<<chunky_scrub>>,5041," __func__ "" waiting_on_whom "" scrubber.waiting_on_whom"
dout,osd/PG.cc,<<chunky_scrub>>,5053," __func__ "" preempted"""
dout,osd/PG.cc,<<chunky_scrub>>,5073," ""error: "" scrubber.primary_scrubmap_pos.ret"
dout,osd/PG.cc,<<chunky_scrub>>,5079," __func__ "" waiting_on_whom was """
dout,osd/PG.cc,<<chunky_scrub>>,5090," ""wait for replicas to build scrub map"""
dout,osd/PG.cc,<<chunky_scrub>>,5097," __func__ "" preempted, restarting chunk"""
dout,osd/PG.cc,<<chunky_scrub>>,5121," __func__ "" waiting on """
dout,osd/PG.cc,<<chunky_scrub>>,5148," ""scrub finished, requeuing snap_trimmer"""
dout,osd/PG.cc,<<chunky_scrub>>,5157," __func__ "" preempted"""
dout,osd/PG.cc,<<chunky_scrub>>,5198," ""scrub final state "" Scrubber::state_string(scrubber.state)"
dout,osd/PG.cc,<<write_blocked_by_scrub>>,5209," __func__ "" "" soid "" preempted"""
dout,osd/PG.cc,<<write_blocked_by_scrub>>,5212," __func__ "" "" soid "" already preempted"""
dout,osd/PG.cc,<<scrub_compare_maps>>,5248," __func__ "" has maps, analyzing"""
dout,osd/PG.cc,<<scrub_compare_maps>>,5261," __func__ "" replica "" i "" has """
dout,osd/PG.cc,<<scrub_compare_maps>>,5284," __func__ "" comparing replica scrub maps"""
dout,osd/PG.cc,<<scrub_compare_maps>>,5289," __func__ "" osd."" acting[0] "" has """
dout,osd/PG.cc,<<scrub_compare_maps>>,5308, ss.str()
dout,osd/PG.cc,<<scrub_compare_maps>>,5348," __func__ "": discarding scrub results"""
dout,osd/PG.cc,<<scrub_compare_maps>>,5351," __func__ "": updating scrub object"""
dout,osd/PG.cc,<<scrub_process_inconsistent>>,5361," __func__ "": checking authoritative"""
dout,osd/PG.cc,<<scrub_process_inconsistent>>,5372, ss.str()
dout,osd/PG.cc,<<share_pg_info>>,5518," ""share_pg_info"""
dout,osd/PG.cc,<<update_history>>,5643," __func__ "" advanced history from "" new_history"
dout,osd/PG.cc,<<update_history>>,5646," __func__ "" clearing past_intervals"""
dout,osd/PG.cc,<<fulfill_log>>,5669," ""log request from "" from"
dout,osd/PG.cc,<<fulfill_log>>,5684," "" sending info+missing+log since "" query.since"
dout,osd/PG.cc,<<fulfill_log>>,5695," "" sending info+missing+full log"""
dout,osd/PG.cc,<<fulfill_log>>,5699," "" sending "" mlog->log "" "" mlog->missing"
dout,osd/PG.cc,<<check_full_transition>>,5710," "" cluster was marked full in "" osdmap->get_epoch()"
dout,osd/PG.cc,<<check_full_transition>>,5720," "" pool was marked full in "" osdmap->get_epoch()"
dout,osd/PG.cc,<<old_peering_msg>>,5762," ""old_peering_msg reply_epoch "" reply_epoch "" query_epoch "" query_epoch"
dout,osd/PG.cc,<<set_last_peering_reset>>,5772," ""set_last_peering_reset "" get_osdmap()->get_epoch()"
dout,osd/PG.cc,<<reset_interval_flush>>,5804," ""Clearing blocked outgoing recovery messages"""
dout,osd/PG.cc,<<reset_interval_flush>>,5810," ""Beginning to block outgoing recovery messages"""
dout,osd/PG.cc,<<reset_interval_flush>>,5813," ""Not blocking outgoing recovery messages"""
dout,osd/PG.cc,<<proc_primary_info>>,6054," __func__ "" updating purged_snaps to "" oinfo.purged_snaps"
dout,osd/PG.cc,<<can_discard_op>>,6172," "" discard "" *m"
dout,osd/PG.cc,<<can_discard_op>>,6177," "" changed after "" m->get_map_epoch()"
dout,osd/PG.cc,<<can_discard_op>>,6184," __func__ "" sent before last_force_op_resend """
dout,osd/PG.cc,<<can_discard_op>>,6189," __func__ "" pg split in """
dout,osd/PG.cc,<<can_discard_op>>,6195," __func__ "" sent before last_force_op_resend_preluminous """
dout,osd/PG.cc,<<can_discard_replica_op>>,6233," ""can_discard_replica_op pg changed "" info.history"
dout,osd/PG.cc,<<can_discard_scan>>,6247," "" got old scan, ignoring"""
dout,osd/PG.cc,<<can_discard_backfill>>,6259," "" got old backfill, ignoring"""
dout,osd/PG.cc,<<take_waiters>>,6324," ""take_waiters"""
dout,osd/PG.cc,<<do_peering_event>>,6330," __func__ "": "" evt->get_desc()"
dout,osd/PG.cc,<<do_peering_event>>,6333," ""discard old "" evt->get_desc()"
dout,osd/PG.cc,<<queue_null>>,6352," ""null"""
dout,osd/PG.cc,<<queue_query>>,6362," ""handle_query "" q "" from replica "" from"
dout,osd/PG.cc,<<find_unfound>>,6400," __func__ "": no luck, giving up on this pg for now ("" action "")"""
dout,osd/PG.cc,<<find_unfound>>,6402," __func__ "": no luck, giving up on this pg for now (queue_recovery)"""
dout,osd/PG.cc,<<handle_activate_map>>,6437," ""handle_activate_map """
dout,osd/PG.cc,<<handle_activate_map>>,6442," __func__ "": Dirtying info: last_persisted is """
dout,osd/PG.cc,<<handle_activate_map>>,6447," __func__ "": Not dirtying info: last_persisted is """
dout,osd/PG.cc,<<handle_initialize>>,6459, __func__
dout,osd/PG.cc,<<handle_query_state>>,6466," ""handle_query_state"""
dout,osd/PG.cc,<<_delete_some>>,6499, __func__
dout,osd/PG.cc,<<_delete_some>>,6512," __func__ "" "" olist"
dout,osd/PG.cc,<<_delete_some>>,6529," __func__ "" deleting "" num "" objects"""
dout,osd/PG.cc,<<_delete_some>>,6533," __func__ "" finished"""
dout,osd/PGBackend.cc,<<recover_delete_object>>,50," __func__ "" will remove "" oid "" "" v "" from """
dout,osd/PGBackend.cc,<<handle_recovery_delete>>,121," __func__ "" "" op"
dout,osd/PGBackend.cc,<<handle_recovery_delete_reply>>,154," __func__ "" "" op"
dout,osd/PGBackend.cc,<<handle_recovery_delete_reply>>,166," __func__ "" "" oid "" still missing on at least """
dout,osd/PGBackend.cc,<<handle_recovery_delete_reply>>,173," __func__ "" completed recovery, local_missing = """
dout,osd/PGBackend.cc,<<on_change_cleanup>>,317, __func__
dout,osd/PGBackend.cc,<<on_change_cleanup>>,322," __func__ "": Removing oid """
dout,osd/PGBackend.cc,<<be_scan_list>>,575," __func__ "" "" pos"
dout,osd/PGBackend.cc,<<be_scan_list>>,600," __func__ "" "" poid"
dout,osd/PGBackend.cc,<<be_scan_list>>,602," __func__ "" "" poid "" got "" r"
dout,osd/PGBackend.cc,<<be_scan_list>>,605," __func__ "" "" poid "" got "" r"
dout,osd/PGLog.cc,<<trim>>,171," ""trim "" log "" to "" trim_to"
dout,osd/PGLog.cc,<<proc_replica_log>>,183," ""proc_replica_log for osd."" from "": """
dout,osd/PGLog.cc,<<proc_replica_log>>,187," __func__ "": osd."" from "" does not overlap, not looking """
dout,osd/PGLog.cc,<<proc_replica_log>>,192," __func__ "": osd."" from "" same log head, not looking """
dout,osd/PGLog.cc,<<proc_replica_log>>,210," "" before missing "" i->first "" need "" i->second.need"
dout,osd/PGLog.cc,<<proc_replica_log>>,220," ""merge_log point (usually last shared) is """
dout,osd/PGLog.cc,<<proc_replica_log>>,257," "" peer osd."" from "" last_update now "" lu"
dout,osd/PGLog.cc,<<rewind_divergent_log>>,291," ""rewind_divergent_log truncate divergent future "" "
dout,osd/PGLog.cc,<<rewind_divergent_log>>,303," ""rewind_divergent_log future divergent "" entry"
dout,osd/PGLog.cc,<<merge_log>>,324," ""merge_log "" olog "" from osd."" fromosd"
dout,osd/PGLog.cc,<<merge_log>>,337," ""pg_missing_t sobject: "" i->first"
dout,osd/PGLog.cc,<<merge_log>>,348," ""merge_log extending tail to "" olog.tail"
dout,osd/PGLog.cc,<<merge_log>>,358, *to
dout,osd/PGLog.cc,<<merge_log>>,388," ""merge_log extending head to "" olog.head"
dout,osd/PGLog.cc,<<merge_log>>,398," "" ? "" *from"
dout,osd/PGLog.cc,<<merge_log>>,405," ""merge_log cut point (usually last shared) is """
dout,osd/PGLog.cc,<<merge_log>>,412," ""merge_log divergent "" oe"
dout,osd/PGLog.cc,<<merge_log>>,453," ""merge_log result "" log "" "" missing "
dout,osd/PGLog.cc,<<merge_log_dups>>,469," ""merge_log copying olog dups to log "" "
dout,osd/PGLog.cc,<<merge_log_dups>>,485," ""merge_log extending dups tail to "" "
dout,osd/PGLog.cc,<<merge_log_dups>>,510," ""merge_log extending dups head to "" "
dout,osd/PGLog.cc,<<merge_log_dups>>,532," ""merge_log removed dups overlapping log entries ["" "
dout,osd/PGLog.cc,<<rebuild_missing_set_with_deletes>>,928," __func__ "" extra missing entry: "" p.first"
dout,osd/PGLog.cc,<<rebuild_missing_set_with_deletes>>,957," __func__ "" check for log entry: "" *i "" = "" r"
dout,osd/PGLog.cc,<<rebuild_missing_set_with_deletes>>,961," __func__ "" store version = "" oi.version"
dout,osd/PrimaryLogPG.cc,<<on_local_recover>>,355," __func__ "": "" hoid"
dout,osd/PrimaryLogPG.cc,<<on_local_recover>>,362," "" snapset "" recovery_info.ss"
dout,osd/PrimaryLogPG.cc,<<on_local_recover>>,366," "" snaps "" snaps"
dout,osd/PrimaryLogPG.cc,<<on_local_recover>>,378," "" got old revert version "" recovery_info.version"
dout,osd/PrimaryLogPG.cc,<<on_local_recover>>,430," "" kicking unreadable waiters on "" hoid"
dout,osd/PrimaryLogPG.cc,<<on_global_recover>>,460," ""pushed "" soid "" to all replicas"""
dout,osd/PrimaryLogPG.cc,<<on_global_recover>>,480," "" kicking degraded waiters on "" soid"
dout,osd/PrimaryLogPG.cc,<<on_global_recover>>,486," "" kicking unreadable waiters on "" soid"
dout,osd/PrimaryLogPG.cc,<<on_primary_error>>,539," __func__ "": oid "" oid "" version "" v"
dout,osd/PrimaryLogPG.cc,<<backfill_add_missing>>,549," __func__ "": oid "" oid "" version "" v"
dout,osd/PrimaryLogPG.cc,<<should_send_op>>,566," __func__ "" issue_repop shipping empty opt to osd."" peer"
dout,osd/PrimaryLogPG.cc,<<should_send_op>>,575," __func__ "" issue_repop shipping empty opt to osd."" peer"
dout,osd/PrimaryLogPG.cc,<<maybe_kick_recovery>>,613," ""object "" soid "" v "" v "", already recovering."""
dout,osd/PrimaryLogPG.cc,<<maybe_kick_recovery>>,615," ""object "" soid "" v "" v "", is unfound."""
dout,osd/PrimaryLogPG.cc,<<maybe_kick_recovery>>,617," ""object "" soid "" v "" v "", recovering."""
dout,osd/PrimaryLogPG.cc,<<is_degraded_on_async_recovery_target>>,687," __func__ "" "" soid"
dout,osd/PrimaryLogPG.cc,<<block_write_on_full_cache>>,707," __func__ "": blocking object "" oid"
dout,osd/PrimaryLogPG.cc,<<block_for_clean>>,717," __func__ "": blocking object "" oid"
dout,osd/PrimaryLogPG.cc,<<block_write_on_snap_rollback>>,726," __func__ "": blocking object "" oid.get_head()"
dout,osd/PrimaryLogPG.cc,<<block_write_on_degraded_snap>>,738," __func__ "": blocking object "" snap.get_head()"
dout,osd/PrimaryLogPG.cc,<<wait_for_blocked_object>>,765," __func__ "" "" soid "" "" op"
dout,osd/PrimaryLogPG.cc,<<maybe_force_recovery>>,803," __func__ "" peer "" peer "" min_version "" min_obj->first"
dout,osd/PrimaryLogPG.cc,<<pgls_filter>>,903," ""getattr (sobj="" sobj "", attr=""...r->get_xattr() "") returned "" ret"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1177," ""do_pg_op "" *m"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1200," ""unable to decode PGLS_FILTER description in "" *m"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1222," "" pgnls pg="" m->get_pg()"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1230," "" pgnls pg="" m->get_pg() "" count "" list_size"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1239," ""unable to decode PGNLS handle in "" *m"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1248," "" pgnls lower_bound "" lower_bound"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1253," ""outside of PG bounds "" pg_start "" .. """
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1302," "" pgnls candidate 0x"" std::hex candidate.get_hash()"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1332," ""pgnls item 0x"" std::hex"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1356," ""pgnls handle="" response.handle"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1360," "" pgnls result="" result "" outdata.length()="""
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1371," ""unable to decode PGLS_FILTER description in "" *m"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1393," "" pgls pg="" m->get_pg()"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1401," "" pgls pg="" m->get_pg() "" count "" list_size"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1409," ""unable to decode PGLS handle in "" *m"
dout,osd/PrimaryLogPG.cc,<<do_pg_op>>,1494," "" pgls result="" result "" outdata.length()="""
dout,osd/PrimaryLogPG.cc,<<do_scrub_ls>>,1581," "" scrubls pg="" m->get_pg() "" != "" info.pgid"
dout,osd/PrimaryLogPG.cc,<<do_scrub_ls>>,1589," "" corrupted scrub_ls_arg_t"""
dout,osd/PrimaryLogPG.cc,<<calc_trim_to>>,1644," ""calc_trim_to trimming to min_last_complete_ondisk"""
dout,osd/PrimaryLogPG.cc,<<calc_trim_to>>,1648," ""calc_trim_to "" pg_trim_to "" -> "" new_trim_to"
dout,osd/PrimaryLogPG.cc,<<handle_backoff>>,1696," __func__ "" backoff ack id "" m->id"
dout,osd/PrimaryLogPG.cc,<<do_request>>,1713," __func__ "" waiting_for_map """
dout,osd/PrimaryLogPG.cc,<<do_request>>,1720," __func__ "" min "" op->min_epoch"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2072," ""do_op "" *m"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2110," __func__ "": waiting for scrub"""
dout,osd/PrimaryLogPG.cc,<<do_request>>,2152," __func__ "" dup "" m->get_reqid()"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2157," "" waiting for "" version "" to commit"""
dout,osd/PrimaryLogPG.cc,<<do_request>>,2184," ""LIST_SNAPS with incorrect context"""
dout,osd/PrimaryLogPG.cc,<<do_request>>,2190," ""non-LIST_SNAPS on snapdir"""
dout,osd/PrimaryLogPG.cc,<<do_request>>,2226," __func__ "": clone "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2236," __func__ "": clone "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2290," __func__ "": find_object_context got error "" r"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2303," "" provided locator "" m->get_object_locator()"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2317," __func__ "" oi "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2322," __func__ "": skipping rw locks"""
dout,osd/PrimaryLogPG.cc,<<do_request>>,2324," __func__ "": part of flush, will ignore write lock"""
dout,osd/PrimaryLogPG.cc,<<do_request>>,2330," __func__ "" no flush in progress, aborting"""
dout,osd/PrimaryLogPG.cc,<<do_request>>,2335," __func__ "" waiting for rw locks """
dout,osd/PrimaryLogPG.cc,<<do_request>>,2340," __func__ "" obc "" *obc"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2343," __func__ "" returned an error: "" r"
dout,osd/PrimaryLogPG.cc,<<do_request>>,2360," __func__ "": object "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<maybe_handle_manifest_detail>>,2406," __func__ "": ignoring redirect due to flag"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_manifest_detail>>,2413," __func__ "" blocked on "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<maybe_handle_manifest_detail>>,2457," __func__ "": "" head "" is degraded, waiting"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_manifest_detail>>,2463," __func__ "": waiting for scrub"""
dout,osd/PrimaryLogPG.cc,<<handle_manifest_flush>>,2523," __func__ "" no flush_op found"""
dout,osd/PrimaryLogPG.cc,<<do_manifest_flush>>,2599," __func__ "" read fail "" "" offset: "" tgt_offset"
dout,osd/PrimaryLogPG.cc,<<do_manifest_flush>>,2624," __func__ "" offset: "" tgt_offset "" len: "" tgt_length"
dout,osd/PrimaryLogPG.cc,<<finish_manifest_flush>>,2638," __func__ "" "" oid "" tid "" tid"
dout,osd/PrimaryLogPG.cc,<<finish_manifest_flush>>,2642," __func__ "" no flush_op found"""
dout,osd/PrimaryLogPG.cc,<<record_write_error>>,2660," __func__ "" r="" r"
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2723," __func__ "": ignoring cache due to flag"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2730," __func__ "" "" obc->obs.oi "" """
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2737," __func__ "" (no obc)"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2746," __func__ "" blocked on "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2760," __func__ "" cache miss; ask the primary"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2784," __func__ "" cache pool full, proxying read"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2788," __func__ "" cache pool full, waiting"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2856," __func__ "" cache pool full, waiting"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2881," __func__ "" cache pool full, waiting"""
dout,osd/PrimaryLogPG.cc,<<maybe_handle_cache_detail>>,2893," __func__ "" cache pool full, waiting"""
dout,osd/PrimaryLogPG.cc,<<maybe_promote>>,2919," __func__ "" missing_oid "" missing_oid"
dout,osd/PrimaryLogPG.cc,<<maybe_promote>>,2962," __func__ "" promote throttled"""
dout,osd/PrimaryLogPG.cc,<<do_cache_redirect>>,2977," ""sending redirect to pool "" pool.info.tier_of "" for op """
dout,osd/PrimaryLogPG.cc,<<do_proxy_read>>,3100," __func__ "" Start proxy read for "" *m"
dout,osd/PrimaryLogPG.cc,<<finish_proxy_read>>,3141," __func__ "" "" oid "" tid "" tid"
dout,osd/PrimaryLogPG.cc,<<finish_proxy_read>>,3146," __func__ "" no proxyread_op found"""
dout,osd/PrimaryLogPG.cc,<<finish_proxy_read>>,3151," __func__ "" tid "" tid "" != prdop "" prdop"
dout,osd/PrimaryLogPG.cc,<<finish_proxy_read>>,3156," __func__ "" oid "" oid "" != prdop "" prdop"
dout,osd/PrimaryLogPG.cc,<<finish_proxy_read>>,3164," __func__ "" no in_progress_proxy_ops found"""
dout,osd/PrimaryLogPG.cc,<<finish_proxy_read>>,3180," __func__ "" "" oid "" is not completed """
dout,osd/PrimaryLogPG.cc,<<kick_proxy_ops_blocked>>,3202," __func__ "" "" soid "" requeuing "" ls.size() "" requests"""
dout,osd/PrimaryLogPG.cc,<<do_proxy_write>>,3309," __func__ "" Start proxy write for "" *m"
dout,osd/PrimaryLogPG.cc,<<do_proxy_chunked_op>>,3386," __func__ "" chunk_index: "" chunks->first"
dout,osd/PrimaryLogPG.cc,<<refcount_manifest>>,3439," __func__ "" Start refcount for "" soid"
dout,osd/PrimaryLogPG.cc,<<do_proxy_chunked_read>>,3490," __func__ "" Start do chunk proxy read for "" *m"
dout,osd/PrimaryLogPG.cc,<<can_proxy_chunked_read>>,3562," __func__ "" requested chunks don't exist in chunk_map """
dout,osd/PrimaryLogPG.cc,<<finish_proxy_write>>,3576," __func__ "" "" oid "" tid "" tid"
dout,osd/PrimaryLogPG.cc,<<finish_proxy_write>>,3581," __func__ "" no proxywrite_op found"""
dout,osd/PrimaryLogPG.cc,<<finish_proxy_write>>,3592," __func__ "" no in_progress_proxy_ops found"""
dout,osd/PrimaryLogPG.cc,<<finish_proxy_write>>,3612," __func__ "" "" oid "" tid "" tid"
dout,osd/PrimaryLogPG.cc,<<finish_proxy_write>>,3633," "" sending commit on "" pwop "" "" reply"
dout,osd/PrimaryLogPG.cc,<<promote_object>>,3735," __func__ "" "" hoid"
dout,osd/PrimaryLogPG.cc,<<promote_object>>,3740," __func__ "" "" hoid"
dout,osd/PrimaryLogPG.cc,<<promote_object>>,3743," __func__ "" "" hoid"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3805," __func__ "" "" ctx"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3831," "" ORDERSNAP flag set and snapc seq "" ctx->snapc.seq"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3842," __func__ "" "" soid "" "" *ctx->ops"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3848," __func__ "" "" soid "" "" *ctx->ops"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3855," __func__ "" user_at_version "" ctx->user_at_version"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3906," "" zeroing write result code "" result"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3936," "" op order client."" n "" tid "" t "" (first)"""
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3939," "" op order client."" n "" tid "" t "" last was "" p->second"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3952," __func__ "" update_log_only -- result="" result"
dout,osd/PrimaryLogPG.cc,<<execute_ctx>>,3988," "" sending reply on "" *m "" "" reply"
dout,osd/PrimaryLogPG.cc,<<log_op_stats>>,4086," ""log_op_stats "" *m"
dout,osd/PrimaryLogPG.cc,<<do_scan>>,4098," ""do_scan "" *m"
dout,osd/PrimaryLogPG.cc,<<do_scan>>,4107," __func__ "": Canceling backfill: Full."""
dout,osd/PrimaryLogPG.cc,<<do_backfill>>,4170," ""do_backfill "" *m"
dout,osd/PrimaryLogPG.cc,<<do_backfill_remove>>,4225," __func__ "" "" m->ls"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4274," coid "" old_snaps "" old_snaps"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4307," __func__ "": Unable to get a wlock on "" coid"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4316," __func__ "": Unable to get a wlock on "" head_oid"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4326," coid "" snaps "" old_snaps "" -> """
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4357," __func__ "" trimming whiteout on "" coid"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4394," coid "" snaps "" old_snaps "" -> "" new_snaps"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4426," coid "" new snapset "" snapset "" on """
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4436," coid "" removing "" head_oid"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4457," __func__ "" trimming whiteout on "" oi.soid"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4469," coid "" filtering snapset on "" head_oid"
dout,osd/PrimaryLogPG.cc,<<trim_object>>,4471," coid "" writing updated snapset on "" head_oid"
dout,osd/PrimaryLogPG.cc,<<kick_snap_trim>>,4510," __func__ "": nosnaptrim set, not kicking"""
dout,osd/PrimaryLogPG.cc,<<kick_snap_trim>>,4512," __func__ "": clean and snaps to trim, kicking"""
dout,osd/PrimaryLogPG.cc,<<snap_trimmer>>,4534," ""snap_trimmer posting"""
dout,osd/PrimaryLogPG.cc,<<snap_trimmer>>,4536," ""snap_trimmer complete"""
dout,osd/PrimaryLogPG.cc,<<do_xattr_cmp_u64>>,4550," ""do_xattr_cmp_u64 '"" v1 ""' vs '"" v2 ""' op "" op"
dout,osd/PrimaryLogPG.cc,<<do_xattr_cmp_str>>,4574," ""do_xattr_cmp_str '"" v1s ""' vs '"" v2s ""' op "" op"
dout,osd/PrimaryLogPG.cc,<<do_tmap2omap>>,4633," "" convert tmap to omap for "" ctx->new_obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4728," ""tmapup is a no-op"""
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4738," ""tmapup read "" newop.outdata.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4740," "" starting is \n"";"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4747," ""the update command is: \n"";"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4758," ""tmapup header "" header.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4763," ""tmapup new header "" header.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4768," ""tmapup initial nkeys "" nkeys"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4791," ""tmapup warning: key '"" key ""' < previous key '"" last_in_key"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4798," ""tmapup op "" (int )op "" key "" key"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4803," "" (have_next="" have_next "" nextkey="" nextkey "")"""
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4810," "" keep "" nextkey "" "" nextval.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4813," "" drop "" nextkey "" "" nextval.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4835," "" set "" key "" "" val.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4850," "" create "" key "" "" val.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4860," "" invalid tmap op "" (int )op"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4869," "" keep "" nextkey "" "" nextval.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4874," "" keep trailing "" rest.length()"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4880," ""tmapup final nkeys "" nkeys"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4885," "" final is \n"";"
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4896," "" **** debug sanity check, looks ok ****"""
dout,osd/PrimaryLogPG.cc,<<do_tmapup>>,4901," ""tmapput write "" obl.length()"
dout,osd/PrimaryLogPG.cc,<<maybe_create_new_object>>,5026," __func__ "" clearing whiteout on "" obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<do_checksum>>,5081, __func__
dout,osd/PrimaryLogPG.cc,<<do_checksum>>,5086," __func__ "": length required when chunk size provided"""
dout,osd/PrimaryLogPG.cc,<<do_checksum>>,5091," __func__ "": length not aligned to chunk size"""
dout,osd/PrimaryLogPG.cc,<<do_checksum>>,5108," __func__ "": length (trimmed to 0x"""
dout,osd/PrimaryLogPG.cc,<<do_checksum>>,5129," __func__ "": unknown crc type ("""
dout,osd/PrimaryLogPG.cc,<<do_checksum>>,5136," __func__ "": init value not provided"""
dout,osd/PrimaryLogPG.cc,<<do_checksum>>,5164," __func__ "": async_read noted for "" soid"
dout,osd/PrimaryLogPG.cc,<<finish_checksum>>,5197, __func__
dout,osd/PrimaryLogPG.cc,<<do_extent_cmp>>,5295, __func__
dout,osd/PrimaryLogPG.cc,<<do_extent_cmp>>,5312," __func__ "" zero length extent"""
dout,osd/PrimaryLogPG.cc,<<do_extent_cmp>>,5315," __func__ "" object DNE"""
dout,osd/PrimaryLogPG.cc,<<do_extent_cmp>>,5334," __func__ "": async_read noted for "" soid"
dout,osd/PrimaryLogPG.cc,<<do_read>>,5372, __func__
dout,osd/PrimaryLogPG.cc,<<do_read>>,5419," "" async_read noted for "" soid"
dout,osd/PrimaryLogPG.cc,<<do_read>>,5446," "" read got "" r "" / "" op.extent.length"
dout,osd/PrimaryLogPG.cc,<<do_sparse_read>>,5458, __func__
dout,osd/PrimaryLogPG.cc,<<do_sparse_read>>,5464," ""sparse_read does not support truncation sequence """
dout,osd/PrimaryLogPG.cc,<<do_sparse_read>>,5487," "" async_read (was sparse_read) noted for "" soid"
dout,osd/PrimaryLogPG.cc,<<do_sparse_read>>,5492," "" sparse read ended up empty for "" soid"
dout,osd/PrimaryLogPG.cc,<<do_sparse_read>>,5543," ""sparse-read "" miter->first ""@"" miter->second"
dout,osd/PrimaryLogPG.cc,<<do_sparse_read>>,5589," "" sparse_read got "" total_read "" bytes from object """
dout,osd/PrimaryLogPG.cc,<<_get_tmap>>,7565," ""unable to get tmap for zero sized "" ctx->new_obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<_get_tmap>>,7577," ""unsuccessful at decoding tmap for "" ctx->new_obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<_get_tmap>>,7581," ""successful at decoding tmap for "" ctx->new_obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<_verify_no_head_clones>>,7590," __func__ "" verifying clones are absent """
dout,osd/PrimaryLogPG.cc,<<_verify_no_head_clones>>,7601," __func__ "" cannot evict head before clone """
dout,osd/PrimaryLogPG.cc,<<_verify_no_head_clones>>,7606," __func__ "" cannot evict head, pending promote on clone """
dout,osd/PrimaryLogPG.cc,<<_delete_oid>>,7639," __func__ "" has or will have clones but no_whiteout=1"""
dout,osd/PrimaryLogPG.cc,<<_delete_oid>>,7642," __func__ "" has or will have clones; will whiteout"""
dout,osd/PrimaryLogPG.cc,<<_delete_oid>>,7647," __func__ "" "" soid "" whiteout="" (int )whiteout"
dout,osd/PrimaryLogPG.cc,<<_delete_oid>>,7677," __func__ "" will disconnect watcher "" p->first"
dout,osd/PrimaryLogPG.cc,<<_delete_oid>>,7684," __func__ "" setting whiteout on "" soid"
dout,osd/PrimaryLogPG.cc,<<_delete_oid>>,7697," __func__ "" deleting whiteout on "" soid"
dout,osd/PrimaryLogPG.cc,<<_rollback_to>>,7721," ""_rollback_to "" soid "" snapid "" snapid"
dout,osd/PrimaryLogPG.cc,<<_rollback_to>>,7732," ""_rollback_to attempted to roll back to a missing or backfilling clone """
dout,osd/PrimaryLogPG.cc,<<_rollback_to>>,7779," ""_rollback_to deleting head on "" soid.oid"
dout,osd/PrimaryLogPG.cc,<<_rollback_to>>,7795," ""_rollback_to attempted to roll back to a degraded object """
dout,osd/PrimaryLogPG.cc,<<_rollback_to>>,7807," ""_rollback_to deleting "" soid.oid"
dout,osd/PrimaryLogPG.cc,<<_rollback_to>>,7848," __func__ "" setting omap flag on "" obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<_rollback_to>>,7851," __func__ "" clearing omap flag on "" obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<make_writeable>>,7881," ""make_writeable "" soid "" snapset="" ctx->new_snapset"
dout,osd/PrimaryLogPG.cc,<<make_writeable>>,7888," "" clearing DIRTY flag"""
dout,osd/PrimaryLogPG.cc,<<make_writeable>>,7894," "" setting DIRTY flag"""
dout,osd/PrimaryLogPG.cc,<<make_writeable>>,7901," "" deletion, decrementing num_dirty and clearing flag"""
dout,osd/PrimaryLogPG.cc,<<make_writeable>>,7921," "" op snapset is old"""
dout,osd/PrimaryLogPG.cc,<<make_writeable>>,7960," "" got greedy write on clone_obc "" *ctx->clone_obc"
dout,osd/PrimaryLogPG.cc,<<make_writeable>>,7993," "" cloning v "" ctx->obs->oi.version"
dout,osd/PrimaryLogPG.cc,<<make_writeable>>,8028," ""make_writeable "" soid"
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8101," ""do_osd_op_effects "" entity "" con "" conn.get()"
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8117," ""do_osd_op_effects applying watch connect on session """
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8121," ""do_osd_op_effects found existing watch watcher "" watcher"
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8125," ""do_osd_op_effects new watcher "" watcher"
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8141," ""do_osd_op_effects, notify "" *p"
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8157," ""starting notify on watch "" i->first"
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8167," ""notify_ack "" make_pair(p->watch_cookie.get(), p->notify_id)"
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8169," ""notify_ack "" make_pair(""NULL"", p->notify_id)"
dout,osd/PrimaryLogPG.cc,<<do_osd_op_effects>>,8177," ""acking notify on watch "" i->first"
dout,osd/PrimaryLogPG.cc,<<generate_temp_object>>,8189," __func__ "" "" hoid"
dout,osd/PrimaryLogPG.cc,<<get_temp_recovery_object>>,8204," __func__ "" "" hoid"
dout,osd/PrimaryLogPG.cc,<<prepare_transaction>>,8214," "" invalid snapc "" ctx->snapc"
dout,osd/PrimaryLogPG.cc,<<prepare_transaction>>,8249," __func__ "" full, but proceeding due to FULL_FORCE or MDS"""
dout,osd/PrimaryLogPG.cc,<<prepare_transaction>>,8253," __func__ "" full, replying to FULL_TRY op"""
dout,osd/PrimaryLogPG.cc,<<prepare_transaction>>,8257," __func__ "" full, dropping request (bad client)"""
dout,osd/PrimaryLogPG.cc,<<finish_ctx>>,8277," __func__ "" "" soid "" "" ctx"
dout,osd/PrimaryLogPG.cc,<<finish_ctx>>,8302," "" set mtime to "" ctx->new_obs.oi.mtime"
dout,osd/PrimaryLogPG.cc,<<finish_ctx>>,8305," "" mtime unchanged at "" ctx->new_obs.oi.mtime"
dout,osd/PrimaryLogPG.cc,<<finish_ctx>>,8317," "" final snapset "" ctx->new_snapset"
dout,osd/PrimaryLogPG.cc,<<finish_ctx>>,8323," "" no snapset (this is a clone)"""
dout,osd/PrimaryLogPG.cc,<<finish_ctx>>,8341," __func__ "" encoding snaps from "" ctx->new_snapset"
dout,osd/PrimaryLogPG.cc,<<finish_ctx>>,8351," __func__ "" extra_reqids "" ctx->extra_reqids"
dout,osd/PrimaryLogPG.cc,<<apply_stats>>,8386," __func__ "" "" soid "" < ["" scrubber.start"
dout,osd/PrimaryLogPG.cc,<<apply_stats>>,8390," __func__ "" "" soid "" >= ["" scrubber.start"
dout,osd/PrimaryLogPG.cc,<<do_copy_get>>,8570," "" got attrs"""
dout,osd/PrimaryLogPG.cc,<<do_copy_get>>,8592," __func__ "": async_read noted for "" soid"
dout,osd/PrimaryLogPG.cc,<<do_copy_get>>,8604," "" got data"""
dout,osd/PrimaryLogPG.cc,<<do_copy_get>>,8641," "" got omap"""
dout,osd/PrimaryLogPG.cc,<<do_copy_get>>,8650," "" got reqids"""
dout,osd/PrimaryLogPG.cc,<<do_copy_get>>,8653," "" cursor.is_complete="" cursor.is_complete()"
dout,osd/PrimaryLogPG.cc,<<fill_in_copy_get_noent>>,8686," __func__ "" got reqids "" reply_obj.reqids"
dout,osd/PrimaryLogPG.cc,<<start_copy>>,8704," __func__ "" "" dest"
dout,osd/PrimaryLogPG.cc,<<_copy_some>>,8744," __func__ "" "" *obc "" "" cop"
dout,osd/PrimaryLogPG.cc,<<_copy_some_manifest>>,8811," __func__ "" "" *obc "" "" cop"
dout,osd/PrimaryLogPG.cc,<<_copy_some_manifest>>,8841," __func__ "" oid "" obc->obs.oi.soid "" num_chunks: "" num_chunks"
dout,osd/PrimaryLogPG.cc,<<_copy_some_manifest>>,8867," __func__ "" tgt_oid: "" soid.oid "" tgt_offset: """
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,8901," __func__ "" "" oid "" tid "" tid"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,8905," __func__ "" no copy_op found"""
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,8910," __func__ "" tid "" tid "" != cop "" cop"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,8936," __func__ "" clone snap "" *p "" has been deleted"""
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,8948," __func__ "" no more snaps for "" oid"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,8986," __func__ "" using temp "" cop->results.temp_oid"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,8995," __func__ "" fetching more"""
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,9002, __func__ std::hex
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,9048," ""fill_in_final_tx: writing """
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,9055," ""fill_in_final_tx: writing to temp object"""
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,9062," __func__ "" success; committing"""
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,9065," __func__ "" complete r = "" cpp_strerror(r)"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk>>,9073," __func__ "" deleting partial temp object """
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk_manifest>>,9092," __func__ "" "" oid "" tid "" tid"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk_manifest>>,9096," __func__ "" no copy_op found"""
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk_manifest>>,9103," __func__ "" tid "" tid "" != cop "" chunk_cop"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk_manifest>>,9135," __func__ "" num_chunk: "" obj_cop->num_chunk"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk_manifest>>,9151," __func__ "" took lock on obc, "" obj_cop->obc->rwstate"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk_manifest>>,9162," __func__ "" offset: "" p.second->cursor.data_offset"
dout,osd/PrimaryLogPG.cc,<<process_copy_chunk_manifest>>,9189," __func__ "" complete r = "" cpp_strerror(r)"
dout,osd/PrimaryLogPG.cc,<<_write_copy_chunk>>,9228," __func__ "" "" cop"
dout,osd/PrimaryLogPG.cc,<<finish_copyfrom>>,9295," ""finish_copyfrom on "" ctx->obs->oi.soid"
dout,osd/PrimaryLogPG.cc,<<finish_copyfrom>>,9299," __func__ "": exists, removing"""
dout,osd/PrimaryLogPG.cc,<<finish_copyfrom>>,9331," __func__ "" clearing whiteout on "" obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<finish_copyfrom>>,9337," __func__ "" setting omap flag on "" obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<finish_copyfrom>>,9340," __func__ "" clearing omap flag on "" obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9364," __func__ "" "" soid "" r="" r"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9388," __func__ "" snaps "" results->snaps"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9391," __func__ "" filtered snaps "" results->snaps"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9393, __func__
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9401," __func__ "" abort; will clean up partial work"""
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9411, __func__
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9446," __func__ "" took lock on obc, "" obc->rwstate"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9457," __func__ "" whiteout "" soid"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9497," __func__ "" creating whiteout on "" soid"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9501," __func__ "" setting omap flag on "" soid"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9544," __func__ "" new_snapset "" tctx->new_snapset"
dout,osd/PrimaryLogPG.cc,<<finish_promote>>,9552," __func__ "" took lock on obc, "" obc->rwstate"
dout,osd/PrimaryLogPG.cc,<<finish_promote_manifest>>,9569," __func__ "" "" soid "" r="" r"
dout,osd/PrimaryLogPG.cc,<<finish_flush>>,9896," __func__ "" "" oid "" tid "" tid"
dout,osd/PrimaryLogPG.cc,<<finish_flush>>,9900," __func__ "" no flush_op found"""
dout,osd/PrimaryLogPG.cc,<<finish_flush>>,9905," __func__ "" tid "" tid "" != fop "" fop"
dout,osd/PrimaryLogPG.cc,<<finish_flush>>,9921," __func__ "" requeueing dups"""
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,9951," __func__ "" flushed_version "" fop->flushed_version"
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,9955," __func__ "" object no longer exists"""
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,9958," __func__ "" requeueing dups"""
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,9976," __func__ "" blocked by scrub"""
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,10001," __func__ "" clearing DIRTY flag for "" oid"
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,10012," __func__ "" took write lock"""
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,10014," __func__ "" waiting on write lock "" fop->op "" """
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,10028," __func__ "" failed write lock, no op; failing"""
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,10074," __func__ "" offset: "" p.second.offset"
dout,osd/PrimaryLogPG.cc,<<try_flush_mark_clean>>,10087," __func__ "" requeueing for "" ctx->at_version"
dout,osd/PrimaryLogPG.cc,<<repop_all_committed>>,10173," __func__ "": repop tid "" repop->rep_tid "" all committed """
dout,osd/PrimaryLogPG.cc,<<op_applied>>,10187," ""op_applied version "" applied_version"
dout,osd/PrimaryLogPG.cc,<<eval_repop>>,10209," ""eval_repop "" *repop"
dout,osd/PrimaryLogPG.cc,<<eval_repop>>,10211," ""eval_repop "" *repop "" (no op)"""
dout,osd/PrimaryLogPG.cc,<<eval_repop>>,10215," "" commit: "" *repop"
dout,osd/PrimaryLogPG.cc,<<eval_repop>>,10238," "" removing "" *repop"
dout,osd/PrimaryLogPG.cc,<<eval_repop>>,10240," "" q front is "" *repop_queue.front()"
dout,osd/PrimaryLogPG.cc,<<issue_repop>>,10261," ""issue_repop rep_tid "" repop->rep_tid"
dout,osd/PrimaryLogPG.cc,<<issue_repop>>,10319," __func__ "" missing_loc before: "" missing_loc.get_locations(soid)"
dout,osd/PrimaryLogPG.cc,<<issue_repop>>,10332," __func__ "" missing_loc after: "" missing_loc.get_locations(soid)"
dout,osd/PrimaryLogPG.cc,<<new_repop>>,10354," ""new_repop rep_tid "" rep_tid "" on "" *ctx->op->get_req()"
dout,osd/PrimaryLogPG.cc,<<new_repop>>,10356," ""new_repop rep_tid "" rep_tid "" (no op)"""
dout,osd/PrimaryLogPG.cc,<<new_repop>>,10368," __func__ "": "" *repop"
dout,osd/PrimaryLogPG.cc,<<remove_repop>>,10400," __func__ "" "" *repop"
dout,osd/PrimaryLogPG.cc,<<simple_opc_create>>,10417," __func__ "" "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<simple_opc_submit>>,10429," __func__ "" "" repop"
dout,osd/PrimaryLogPG.cc,<<check_blacklisted_watchers>>,10592," ""PrimaryLogPG::check_blacklisted_watchers for pg "" get_pgid()"
dout,osd/PrimaryLogPG.cc,<<check_blacklisted_obc_watchers>>,10600," ""PrimaryLogPG::check_blacklisted_obc_watchers for obc "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<check_blacklisted_obc_watchers>>,10607," ""watch: Found "" j->second->get_en... "" cookie "" j->second->get_cookie()"
dout,osd/PrimaryLogPG.cc,<<check_blacklisted_obc_watchers>>,10609," ""watch: Check entity_addr_t "" ea"
dout,osd/PrimaryLogPG.cc,<<check_blacklisted_obc_watchers>>,10611," ""watch: Found blacklisted watcher for "" ea"
dout,osd/PrimaryLogPG.cc,<<populate_obc_watchers>>,10631," ""populate_obc_watchers "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<populate_obc_watchers>>,10640," "" unconnected watcher "" p->first "" will expire "" expire"
dout,osd/PrimaryLogPG.cc,<<handle_watch_timeout>>,10658," ""handle_watch_timeout obc "" obc"
dout,osd/PrimaryLogPG.cc,<<handle_watch_timeout>>,10661," ""handle_watch_timeout not active, no-op"""
dout,osd/PrimaryLogPG.cc,<<handle_watch_timeout>>,10668," ""handle_watch_timeout waiting for degraded on obj """
dout,osd/PrimaryLogPG.cc,<<handle_watch_timeout>>,10675," ""handle_watch_timeout waiting for scrub on obj """
dout,osd/PrimaryLogPG.cc,<<create_object_context>>,10731," ""create_object_context "" (void *)obc.get() "" "" oi.soid "" """
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10887," __func__ "" "" oid"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10900," __func__ "" snap "" oid.snap "" is removed"""
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10906," __func__ "" "" oid "" no snapset"""
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10915," __func__ "" "" oid "" @"" oid.snap"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10921," __func__ "" "" oid "" @"" oid.snap"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10933," __func__ "" "" oid "" @"" oid.snap"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10940," __func__ "" "" oid "" @"" oid.snap"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10945," __func__ "" "" oid "" @"" oid.snap"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10956," __func__ "" "" oid "" @"" oid.snap"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10964," __func__ "" "" oid "" @"" oid.snap"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10974," __func__ "" "" oid "" @"" oid.snap"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,10980," __func__ "" "" head"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11000," __func__ "" no clones with last >= oid.snap """
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11009," __func__ "" "" soid "" missing, try again later"""
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11023," __func__ "" clone is degraded or backfilling "" soid"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11026," __func__ "" clone is recovering "" soid"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11029," __func__ "" missing clone "" soid"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11043," __func__ "" "" soid"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11050," __func__ "" "" soid "" empty snapset -- DNE"""
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11057," __func__ "" "" soid "" ["" first "","" last"
dout,osd/PrimaryLogPG.cc,<<find_object_context>>,11062," __func__ "" "" soid "" ["" first "","" last"
dout,osd/PrimaryLogPG.cc,<<add_object_context_to_pg_stat>>,11078," __func__ "" "" oi.soid"
dout,osd/PrimaryLogPG.cc,<<kick_object_context_blocked>>,11113," __func__ "" "" soid "" still blocked"""
dout,osd/PrimaryLogPG.cc,<<kick_object_context_blocked>>,11120," __func__ "" "" soid "" requeuing "" ls.size() "" requests"""
dout,osd/PrimaryLogPG.cc,<<recover_missing>>,11213," __func__ "" "" soid"
dout,osd/PrimaryLogPG.cc,<<recover_missing>>,11233," __func__ "": soid "" soid "" needs to be deleted from replica "" shard"
dout,osd/PrimaryLogPG.cc,<<recover_missing>>,11261," "" missing but already recovering head "" head"
dout,osd/PrimaryLogPG.cc,<<remove_missing_object>>,11295," __func__ "" "" soid "" "" v"
dout,osd/PrimaryLogPG.cc,<<finish_degraded_object>>,11327," __func__ "" "" oid"
dout,osd/PrimaryLogPG.cc,<<_committed_pushed_object>>,11350," __func__ "" last_complete "" last_complete "" now ondisk"""
dout,osd/PrimaryLogPG.cc,<<_committed_pushed_object>>,11370," __func__ "" pg has changed, not touching last_complete_ondisk"""
dout,osd/PrimaryLogPG.cc,<<_applied_recovered_object>>,11378, __func__
dout,osd/PrimaryLogPG.cc,<<_applied_recovered_object>>,11380," ""obc = "" *obc"
dout,osd/PrimaryLogPG.cc,<<_applied_recovered_object_replica>>,11394, __func__
dout,osd/PrimaryLogPG.cc,<<recover_got>>,11417," ""got missing "" oid "" v "" v"
dout,osd/PrimaryLogPG.cc,<<recover_got>>,11420," ""last_complete now "" info.last_complete"
dout,osd/PrimaryLogPG.cc,<<recover_got>>,11424," ""last_complete now "" info.last_complete"
dout,osd/PrimaryLogPG.cc,<<pick_newest_available>>,11464," ""pick_newest_available "" oid ""... on osd."" osd->whoami "" (local)"""
dout,osd/PrimaryLogPG.cc,<<pick_newest_available>>,11476," ""pick_newest_available "" oid "" "" h "" on osd."" peer"
dout,osd/PrimaryLogPG.cc,<<pick_newest_available>>,11481," ""pick_newest_available "" oid "" "" v "" (newest)"""
dout,osd/PrimaryLogPG.cc,<<do_update_log_missing>>,11497," __func__ "" op_trim_to = "" op_t...l_forward_to = "" op_roll_forward_to"
dout,osd/PrimaryLogPG.cc,<<do_update_log_missing_reply>>,11550," __func__ "" got reply from """
dout,osd/PrimaryLogPG.cc,<<mark_all_unfound_lost>>,11585," __func__ "" "" pg_log_entry_t::get_op_name(what)"
dout,osd/PrimaryLogPG.cc,<<mark_all_unfound_lost>>,11588," __func__ "": log before:\n"";"
dout,osd/PrimaryLogPG.cc,<<mark_all_unfound_lost>>,11630, e
dout,osd/PrimaryLogPG.cc,<<mark_all_unfound_lost>>,11649, e
dout,osd/PrimaryLogPG.cc,<<mark_all_unfound_lost>>,11708," ""do_command r="" 0 "" "" rs"
dout,osd/PrimaryLogPG.cc,<<apply_and_flush_repops>>,11736," "" canceling repop tid "" repop->rep_tid"
dout,osd/PrimaryLogPG.cc,<<apply_and_flush_repops>>,11743," "" requeuing "" *repop->op->get_req()"
dout,osd/PrimaryLogPG.cc,<<apply_and_flush_repops>>,11752," "" also requeuing ondisk waiters "" p->second"
dout,osd/PrimaryLogPG.cc,<<on_removal>>,11808, __func__
dout,osd/PrimaryLogPG.cc,<<clear_async_reads>>,11824, __func__
dout,osd/PrimaryLogPG.cc,<<clear_async_reads>>,11826," ""clear ctx: """
dout,osd/PrimaryLogPG.cc,<<on_shutdown>>,11836, __func__
dout,osd/PrimaryLogPG.cc,<<on_activate>>,11882," ""activate not all replicas are up-to-date, queueing recovery"""
dout,osd/PrimaryLogPG.cc,<<on_activate>>,11890," ""activate queueing backfill"""
dout,osd/PrimaryLogPG.cc,<<on_activate>>,11898," ""activate all replicas clean, no recovery"""
dout,osd/PrimaryLogPG.cc,<<on_activate>>,11914," __func__ "": bft="" backfill_targets"
dout,osd/PrimaryLogPG.cc,<<on_activate>>,11919," ""target shard "" *i"
dout,osd/PrimaryLogPG.cc,<<_on_new_interval>>,11931," __func__ "" checking missing set d...g. missing = "" pg_log.get_missing()"
dout,osd/PrimaryLogPG.cc,<<on_change>>,11941, __func__
dout,osd/PrimaryLogPG.cc,<<on_change>>,11944," "" discarding empty hit_set"""
dout,osd/PrimaryLogPG.cc,<<on_role_change>>,12051, __func__
dout,osd/PrimaryLogPG.cc,<<on_role_change>>,12053," "" clearing hit set"""
dout,osd/PrimaryLogPG.cc,<<on_pool_change>>,12060, __func__
dout,osd/PrimaryLogPG.cc,<<on_pool_change>>,12068," __func__ "" requeuing full waiters (not in writeback) """
dout,osd/PrimaryLogPG.cc,<<cancel_pull>>,12108," __func__ "": "" soid"
dout,osd/PrimaryLogPG.cc,<<cancel_pull>>,12120," "" kicking degraded waiters on "" soid"
dout,osd/PrimaryLogPG.cc,<<cancel_pull>>,12125," "" kicking unreadable waiters on "" soid"
dout,osd/PrimaryLogPG.cc,<<check_recovery_sources>>,12147," ""peer_log_requested removing "" *i"
dout,osd/PrimaryLogPG.cc,<<check_recovery_sources>>,12158," ""peer_missing_requested removing "" *i"
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12186," ""recovery raced and were queued twice, ignoring!"""
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12223," ""deferring backfill due to NOBACKFILL"""
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12227," ""deferring backfill due to NOREBALANCE"""
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12230," ""deferring backfill due to !backfill_reserved"""
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12232," ""queueing RequestBackfill"""
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12247," "" started "" started"
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12257," __func__ "" needs_recovery: """
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12260," __func__ "" missing_loc: """
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12265," "" still have "" unfound "" unfound"""
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12288," ""recovery done, queuing backfill"""
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12296," ""recovery done, no backfill"""
dout,osd/PrimaryLogPG.cc,<<start_recovery_ops>>,12310," ""recovery done, backfill done"""
dout,osd/PrimaryLogPG.cc,<<recover_primary>>,12333," __func__ "" recovering "" recovering.size()"
dout,osd/PrimaryLogPG.cc,<<recover_primary>>,12337," __func__ "" "" missing.get_items()"
dout,osd/PrimaryLogPG.cc,<<recover_primary>>,12368," __func__ "" """
dout,osd/PrimaryLogPG.cc,<<recover_primary>>,12393," "" already reverting "" soid"
dout,osd/PrimaryLogPG.cc,<<recover_primary>>,12395," "" reverting "" soid "" to "" latest->prior_version"
dout,osd/PrimaryLogPG.cc,<<recover_primary>>,12432," "" need to pull prior_version "" alternate_need "" for revert "" item"
dout,osd/PrimaryLogPG.cc,<<recover_primary>>,12441," "" will pull "" alternate_need "" or "" need"
dout,osd/PrimaryLogPG.cc,<<primary_error>>,12497," info.pgid "" unexpectedly missing "" soid "" v"" v"
dout,osd/PrimaryLogPG.cc,<<prep_object_replica_deletes>>,12516," __func__ "": on "" soid"
dout,osd/PrimaryLogPG.cc,<<prep_object_replica_deletes>>,12521," ""replica delete delayed on "" soid"
dout,osd/PrimaryLogPG.cc,<<prep_object_replica_deletes>>,12526," ""replica delete got recovery read lock on "" soid"
dout,osd/PrimaryLogPG.cc,<<prep_object_replica_pushes>>,12548," __func__ "": on "" soid"
dout,osd/PrimaryLogPG.cc,<<prep_object_replica_pushes>>,12558," ""recovery delayed on "" soid"
dout,osd/PrimaryLogPG.cc,<<prep_object_replica_pushes>>,12563," ""recovery got recovery read lock on "" soid"
dout,osd/PrimaryLogPG.cc,<<prep_object_replica_pushes>>,12583," __func__ "" Error "" r "" on oid "" soid"
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12594," __func__ ""("" max "")"""
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12632," "" peer osd."" peer "" missing "" m_sz "" objects."""
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12633," "" peer osd."" peer "" missing "" pm->second.get_items()"
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12644," __func__ "": "" soid "" still unfound"""
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12659," __func__ "": already recovering "" soid"
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12664," __func__ "": "" soid "" is a delete, removing"""
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12671," __func__ "": "" soid.get_head()"
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12677," __func__ "": "" soid "" still missing on primary"""
dout,osd/PrimaryLogPG.cc,<<recover_replicas>>,12681," __func__ "": recover_object_replicas("" soid "")"""
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12759," __func__ "" ("" max "")"""
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12787," ""peer osd."" *i"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12822," "" my backfill interval "" backfill_info"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12831," "" peer shard "" bt "" backfill "" pbi"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12834," "" scanning peer osd."" bt "" from "" pbi.end"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12855," "" reached end for both local and all peers"""
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12876," "" BACKFILL removing "" check"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12926," "" BACKFILL keeping "" check"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12936," "" BACKFILL replacing "" check"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12941," "" BACKFILL pushing "" backfill_info.begin"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12952," __func__ "" Error "" r "" trying to backfill "" backfill_info.begin"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12958," ""backfill blocking on "" backfill_info.begin"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12963," ""need_ver_targs="" need_ver_targs"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,12965," ""backfill_targets="" backfill_targets"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,13023," ""backfill_pos is "" backfill_pos"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,13027," *i "" is still in flight"""
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,13033," ""starting new_last_backfill at "" new_last_backfill"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,13039," "" pending_backfill_update "" i->first"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,13052," ""possible new_last_backfill at "" new_last_backfill"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,13062," ""final new_last_backfill at "" new_last_backfill"
dout,osd/PrimaryLogPG.cc,<<recover_backfill>>,13100," "" peer "" bt"
dout,osd/PrimaryLogPG.cc,<<update_range>>,13157," __func__ "": bi is old, rescanning local backfill_info"""
dout,osd/PrimaryLogPG.cc,<<update_range>>,13164," __func__ "": bi is current """
dout,osd/PrimaryLogPG.cc,<<update_range>>,13178," __func__ "": bi is old, ("" bi->version"
dout,osd/PrimaryLogPG.cc,<<update_range>>,13183," __func__ "": updating from version "" e.version"
dout,osd/PrimaryLogPG.cc,<<update_range>>,13189," __func__ "": "" e.soid "" updated to version """
dout,osd/PrimaryLogPG.cc,<<update_range>>,13197," __func__ "": "" e.soid "" removed"""
dout,osd/PrimaryLogPG.cc,<<update_range>>,13202," ""scanning pg log first"""
dout,osd/PrimaryLogPG.cc,<<update_range>>,13204," ""scanning projected log"""
dout,osd/PrimaryLogPG.cc,<<scan_range>>,13217," ""scan_range from "" bi->begin"
dout,osd/PrimaryLogPG.cc,<<scan_range>>,13224," "" got "" ls.size() "" items, next "" bi->end"
dout,osd/PrimaryLogPG.cc,<<scan_range>>,13225, ls
dout,osd/PrimaryLogPG.cc,<<scan_range>>,13234," "" "" *p "" "" obc->obs.oi.version"
dout,osd/PrimaryLogPG.cc,<<scan_range>>,13249," "" "" *p "" "" oi.version"
dout,osd/PrimaryLogPG.cc,<<check_local>>,13261, __func__
dout,osd/PrimaryLogPG.cc,<<check_local>>,13278," "" checking "" p->soid"
dout,osd/PrimaryLogPG.cc,<<get_hit_set_current_object>>,13308," __func__ "" "" hoid"
dout,osd/PrimaryLogPG.cc,<<get_hit_set_archive_object>>,13328," __func__ "" "" hoid"
dout,osd/PrimaryLogPG.cc,<<hit_set_clear>>,13334, __func__
dout,osd/PrimaryLogPG.cc,<<hit_set_create>>,13410," __func__ "" "" params"
dout,osd/PrimaryLogPG.cc,<<hit_set_create>>,13425," __func__ "" previous set had approx "" unique"
dout,osd/PrimaryLogPG.cc,<<hit_set_create>>,13440," __func__ "" target_size "" p->target_size"
dout,osd/PrimaryLogPG.cc,<<hit_set_apply_log>>,13461," __func__ "" no update"""
dout,osd/PrimaryLogPG.cc,<<hit_set_apply_log>>,13465," __func__ "" "" to "" .. "" info.last_update"
dout,osd/PrimaryLogPG.cc,<<hit_set_persist>>,13479, __func__
dout,osd/PrimaryLogPG.cc,<<hit_set_persist>>,13513," __func__ "" backfill target osd."" *p"
dout,osd/PrimaryLogPG.cc,<<hit_set_persist>>,13535," __func__ "" archive "" oid"
dout,osd/PrimaryLogPG.cc,<<hit_set_trim>>,13619," __func__ "" removing "" oid"
dout,osd/PrimaryLogPG.cc,<<agent_setup>>,13676," __func__ "" allocated new state, position """
dout,osd/PrimaryLogPG.cc,<<agent_setup>>,13679," __func__ "" keeping existing state"""
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13700," __func__ "" no agent state, stopping"""
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13708," __func__ "" idle, stopping"""
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13715, __func__
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13742," __func__ "" got "" ls.size() "" objects"""
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13748," __func__ "" skip (hit set) "" *p"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13753," __func__ "" skip (degraded) "" *p"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13758," __func__ "" skip (missing head) "" *p"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13765," __func__ "" skip (no obc) "" *p"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13770," __func__ "" skip (dne) "" obc->obs.oi.soid"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13776," __func__ "" skip (scrubbing) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13781," __func__ "" skip (blocked) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13786," __func__ "" skip (request pending) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13794," __func__ "" skip (omap to EC) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13816," __func__ "" resetting atime and temp histograms"""
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13825," __func__ "" start pos "" agent_state->position"
dout,osd/PrimaryLogPG.cc,<<agent_work>>,13834," __func__ "" wrap around "" agent_state->start"
dout,osd/PrimaryLogPG.cc,<<agent_load_hit_sets>>,13870, __func__
dout,osd/PrimaryLogPG.cc,<<agent_load_hit_sets>>,13874," __func__ "" loading "" p->begin ""-"""
dout,osd/PrimaryLogPG.cc,<<agent_load_hit_sets>>,13884," __func__ "" unreadable "" oid "", waiting"""
dout,osd/PrimaryLogPG.cc,<<agent_maybe_flush>>,13911," __func__ "" skip (clean) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_flush>>,13916," __func__ "" skip (cache_pinned) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_flush>>,13933," __func__ "" skip (too young) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_flush>>,13939," __func__ "" skip (flushing) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_flush>>,13944," __func__ "" flushing "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_flush>>,13961," __func__ "" start_flush() failed "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,13975," __func__ "" skip (dirty) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,13979," __func__ "" skip (watchers) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,13983," __func__ "" skip (blocked) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,13987," __func__ "" skip (cache_pinned) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,13994," __func__ "" skip (clones) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,14009," __func__ "" skip (too young) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,14021, __func__
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,14026," ""agent_state:\n"";"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,14039," __func__ "" evicting "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_maybe_evict>>,14049," __func__ "" skip (cannot get lock) "" obc->obs.oi"
dout,osd/PrimaryLogPG.cc,<<agent_stop>>,14078, __func__
dout,osd/PrimaryLogPG.cc,<<agent_delay>>,14088, __func__
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode_restart>>,14098, __func__
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode>>,14112," __func__ "" "" this "" delaying, ignored"""
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode>>,14122," __func__ "" stats invalid (post-split), idle"""
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode>>,14162, __func__
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode>>,14203," __func__ "" dirty "" ((float )dirty_micro / 1000000.0)"
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode>>,14254," __func__ "" evict_effort "" was ... by "" inc "" to "" evict_effort"
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode>>,14261," __func__ "" flush_mode """
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode>>,14281," __func__ "" evict_mode """
dout,osd/PrimaryLogPG.cc,<<agent_choose_mode>>,14311," __func__ "" evict_effort """
dout,osd/PrimaryLogPG.cc,<<already_complete>>,14359," __func__ "": "" v"
dout,osd/PrimaryLogPG.cc,<<already_complete>>,14363," __func__ "": "" **i"
dout,osd/PrimaryLogPG.cc,<<already_complete>>,14366," __func__ "": "" **i"
dout,osd/PrimaryLogPG.cc,<<already_complete>>,14371," __func__ "": "" **i"
dout,osd/PrimaryLogPG.cc,<<already_complete>>,14376," __func__ "": "" **i"
dout,osd/PrimaryLogPG.cc,<<already_complete>>,14382," __func__ "": returning true"""
dout,osd/PrimaryLogPG.cc,<<already_ack>>,14388," __func__ "": "" v"
dout,osd/PrimaryLogPG.cc,<<already_ack>>,14394," __func__ "": "" **i"
dout,osd/PrimaryLogPG.cc,<<already_ack>>,14399," __func__ "": "" **i"
dout,osd/PrimaryLogPG.cc,<<already_ack>>,14404," __func__ "": returning true"""
dout,osd/PrimaryLogPG.cc,<<_range_available_for_scrub>>,14423," __func__ "": scrub delayed, """
dout,osd/PrimaryLogPG.cc,<<_scrub_finish>>,14867," mode "" got """
dout,osd/PrimaryLogPG.cc,<<rep_repair_primary_object>>,14938," __func__ "" "" soid"
dout,osd/PrimaryLogPG.cc,<<rep_repair_primary_object>>,14953," __func__ "": Need version of replica, objects_get_attr failed: """
dout,osd/PrimaryLogPG.cc,<<rep_repair_primary_object>>,14962," __func__ "": Need version of replica, bad object_info_t: "" soid"
dout,osd/PrimaryLogPG.cc,<<rep_repair_primary_object>>,14967," __func__ "" No other replicas available for "" soid"
dout,osd/PrimaryLogPG.cc,<<rep_repair_primary_object>>,14993," __func__ "": Read error on "" soid "", but already seen errors"""
dout,osd/ReplicatedBackend.cc,<<recover_object>>,128," __func__ "": "" hoid"
dout,osd/ReplicatedBackend.cc,<<check_recovery_sources>>,158," ""check_recovery_sources resetting pulls from osd."" i->first"
dout,osd/ReplicatedBackend.cc,<<can_handle_while_inactive>>,175," __func__ "": "" op"
dout,osd/ReplicatedBackend.cc,<<_handle_message>>,188," __func__ "": "" op"
dout,osd/ReplicatedBackend.cc,<<on_change>>,237, __func__
dout,osd/ReplicatedBackend.cc,<<op_commit>>,511," __func__ "": "" op->tid"
dout,osd/ReplicatedBackend.cc,<<do_repop_reply>>,547," __func__ "": tid "" ip_op.tid "" op """
dout,osd/ReplicatedBackend.cc,<<do_repop_reply>>,552," __func__ "": tid "" ip_op.tid "" (no op) """
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,591," __func__ "" "" poid "" pos "" pos"
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,621," __func__ "" "" poid "" got """
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,631," __func__ "" "" poid "" more data, digest so far 0x"""
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,641," __func__ "" "" poid "" done with data, digest 0x"""
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,656," __func__ "" "" poid "" got """
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,662," ""CRC header "" string(hdrbl.c_str(), hdrbl.length())"
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,697," __func__ "" "" poid"
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,708," __func__ "" "" poid"
dout,osd/ReplicatedBackend.cc,<<be_deep_scrub>>,719," __func__ "" done with "" poid "" omap_digest """
dout,osd/ReplicatedBackend.cc,<<_do_push>>,737," __func__ "" Out of space (failsafe) processing push request."""
dout,osd/ReplicatedBackend.cc,<<_do_pull_response>>,802," __func__ "" Out of space (failsafe) processing pull response (push)."""
dout,osd/ReplicatedBackend.cc,<<do_repop>>,998," __func__ "" "" soid"
dout,osd/ReplicatedBackend.cc,<<do_repop>>,1007," __func__ "" missing before "" ge...)->get_log().get_missing().get_items()"
dout,osd/ReplicatedBackend.cc,<<do_repop>>,1028," __func__ "" start tracking temp "" m->new_temp_oid"
dout,osd/ReplicatedBackend.cc,<<do_repop>>,1032," __func__ "" stop tracking temp "" m->discard_temp_oid"
dout,osd/ReplicatedBackend.cc,<<do_repop>>,1034," __func__ "": removing object "" m->discard_temp_oid"
dout,osd/ReplicatedBackend.cc,<<do_repop>>,1056," __func__ "" is_missing "" pmissing.is_missing(soid)"
dout,osd/ReplicatedBackend.cc,<<do_repop>>,1058," "" add_next_event entry "" e"
dout,osd/ReplicatedBackend.cc,<<do_repop>>,1060," "" entry is_delete "" e.is_delete()"
dout,osd/ReplicatedBackend.cc,<<do_repop>>,1082," __func__ "" missing after"" get_...)->get_log().get_missing().get_items()"
dout,osd/ReplicatedBackend.cc,<<repop_commit>>,1094," __func__ "" on op "" *m"
dout,osd/ReplicatedBackend.cc,<<prepare_pull>>,1296," ""pull "" soid"
dout,osd/ReplicatedBackend.cc,<<prepare_pull>>,1306," ""pulling soid "" soid "" from osd "" fromshard"
dout,osd/ReplicatedBackend.cc,<<prepare_pull>>,1327," "" snapset "" ssc->snapset"
dout,osd/ReplicatedBackend.cc,<<prepare_pull>>,1336," "" pulling "" recovery_info"
dout,osd/ReplicatedBackend.cc,<<prep_push_to_replica>>,1382," __func__ "": "" soid "" v"" oi.version"
dout,osd/ReplicatedBackend.cc,<<prep_push_to_replica>>,1397," ""push_to_replica missing head "" head "", pushing raw clone"""
dout,osd/ReplicatedBackend.cc,<<prep_push_to_replica>>,1403," ""push_to_replica snapset is "" ssc->snapset"
dout,osd/ReplicatedBackend.cc,<<prep_push_to_replica>>,1422," ""push_to_replica snapset is "" ssc->snapset"
dout,osd/ReplicatedBackend.cc,<<submit_push_complete>>,1575," "" clone_range "" p->first "" """
dout,osd/ReplicatedBackend.cc,<<handle_push>>,1713," ""handle_push """
dout,osd/ReplicatedBackend.cc,<<build_push_op>>,1818," __func__ "" "" recovery_info.soid"
dout,osd/ReplicatedBackend.cc,<<build_push_op>>,1828," __func__ "" get omap header failed: "" cpp_strerror(-r)"
dout,osd/ReplicatedBackend.cc,<<build_push_op>>,1833," __func__ "" getattrs failed: "" cpp_strerror(-r)"
dout,osd/ReplicatedBackend.cc,<<build_push_op>>,1844," __func__ "": bad object_info_t: "" recovery_info.soid"
dout,osd/ReplicatedBackend.cc,<<build_push_op>>,1927," __func__ "": inject EIO "" recovery_info.soid"
dout,osd/ReplicatedBackend.cc,<<build_push_op>>,1934," "" extent "" p.get_start() ""~"" p.get_len()"
dout,osd/ReplicatedBackend.cc,<<handle_push_reply>>,1989," ""huh, i wasn't pushing "" soid "" to osd."" peer"
dout,osd/ReplicatedBackend.cc,<<handle_push_reply>>,1994," ""huh, i wasn't pushing "" soid "" to osd."" peer"
dout,osd/ReplicatedBackend.cc,<<handle_push_reply>>,2002," "" pushing more from, """
dout,osd/ReplicatedBackend.cc,<<handle_push_reply>>,2013," __func__ "": oid "" soid "" error "" r"
dout,osd/ReplicatedBackend.cc,<<handle_push_reply>>,2043," ""pushed "" soid "", still waiting for push ack from """
dout,osd/ReplicatedBackend.cc,<<_failed_pull>>,2125," __func__ "": "" soid "" from "" from"
dout,osd/ReplicatedBackend.cc,<<start_pushes>>,2159," __func__ "" soid "" soid"
dout,osd/ReplicatedBackend.cc,<<start_pushes>>,2188," __func__ "" clean up peer "" p"
dout,osd/Session.cc,<<ack_backoff>>,50," __func__ "" "" pgid "" "" id "" ["" begin "","""
dout,osd/Session.cc,<<ack_backoff>>,56," __func__ "" "" pgid "" "" id "" ["" begin "","""
dout,osd/Session.cc,<<ack_backoff>>,65," __func__ "" now "" *b"
dout,osd/Session.cc,<<ack_backoff>>,67," __func__ "" deleting "" *b"
dout,osd/Session.cc,<<ack_backoff>>,75," __func__ "" clearing begin bin "" q->first"
dout,osd/Session.cc,<<ack_backoff>>,78," __func__ "" clearing pg bin "" p->first"
dout,osd/Session.cc,<<check_backoff>>,90," __func__ "" session "" this "" has backoff "" *b"
dout,osd/Session.cc,<<check_backoff>>,99," __func__ "" session "" this "" disconnected"""
dout,osd/SnapMapper.cc,<<get_snaps>>,158," __func__ "" "" oid "" got err "" r"
dout,osd/SnapMapper.cc,<<get_snaps>>,162," __func__ "" "" oid "" got.empty()"""
dout,osd/SnapMapper.cc,<<get_snaps>>,168," __func__ "" "" oid "" "" out->snaps"
dout,osd/SnapMapper.cc,<<get_snaps>>,170," __func__ "" "" oid "" empty snapset"""
dout,osd/SnapMapper.cc,<<get_snaps>>,174," __func__ "" "" oid "" (out == NULL)"""
dout,osd/Watch.cc,<<do_timeout>>,95," ""timeout"""
dout,osd/Watch.cc,<<start_watcher>>,151," ""start_watcher"""
dout,osd/Watch.cc,<<complete_watcher>>,158," ""complete_watcher"""
dout,osd/Watch.cc,<<complete_watcher_remove>>,172, __func__
dout,osd/Watch.cc,<<maybe_complete_notify>>,182," ""maybe_complete_notify -- """
dout,osd/Watch.cc,<<finish>>,269," ""HandleWatchTimeoutDelayed"""
dout,osd/Watch.cc,<<Watch>>,304," ""Watch()"""
dout,osd/Watch.cc,<<discarded>>,304," ""Watch()"""
dout,osd/Watch.cc,<<Watch>>,308," ""~Watch"""
dout,osd/Watch.cc,<<register_cb>>,327," ""re-registering callback, timeout: "" timeout"
dout,osd/Watch.cc,<<register_cb>>,331," ""registering callback, timeout: "" timeout"
dout,osd/Watch.cc,<<unregister_cb>>,341," ""unregister_cb"""
dout,osd/Watch.cc,<<unregister_cb>>,344," ""actually registered, cancelling"""
dout,osd/Watch.cc,<<connect>>,364," __func__ "" con "" con "" - already connected"""
dout,osd/Watch.cc,<<connect>>,367," __func__ "" con "" con"
dout,osd/Watch.cc,<<disconnect>>,390," ""disconnect (con was "" conn "")"""
dout,osd/Watch.cc,<<discard>>,398," ""discard"""
dout,osd/Watch.cc,<<remove>>,433," ""remove"""
dout,osd/Watch.cc,<<start_notify>>,456," __func__ "" "" notif->notify_id"
dout,osd/Watch.cc,<<start_notify>>,463," ""start_notify "" notif->notify_id"
dout,osd/Watch.cc,<<cancel_notify>>,472," ""cancel_notify "" notif->notify_id"
dout,osd/Watch.cc,<<send_notify>>,478," ""send_notify"""
dout,osd/Watch.cc,<<notify_ack>>,488," ""notify_ack"""
